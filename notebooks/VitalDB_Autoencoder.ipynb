{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vitaldb\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_healthy_API(type='n',n_cases=None):\n",
    "    caseids_all = vitaldb.find_cases(['ECG_II','ART_DBP','ART_SBP','BT','HR','RR']) # find ids of patient with this parameters\n",
    "    # Load dataset\n",
    "    df = pd.read_csv('https://api.vitaldb.net/cases')\n",
    "    df = df[df['asa'] < 3]\n",
    "\n",
    "    caseids_unhealthy = df['caseid'].to_numpy() \n",
    "    caseids = [el for el in caseids_all if el in caseids_unhealthy]\n",
    "\n",
    "    if(n_cases is None):\n",
    "        n_cases = len(caseids)\n",
    "\n",
    "    ecg = []\n",
    "    dbp = []\n",
    "    sbp = []\n",
    "    bt  = []\n",
    "    hr  = []\n",
    "    rr  = []\n",
    "\n",
    "    if(type in ('a','n','w')):\n",
    "\n",
    "        if(type in ('a','n')):\n",
    "\n",
    "            # load all the patients data \n",
    "            for i in range(0,n_cases): # Select only five patient for testing purpose; then len(caseids)\n",
    "                try:\n",
    "                    vals = vitaldb.load_case(caseids[i], ['ART_DBP','ART_SBP','BT','HR','RR'])\n",
    "                    dbp.append(vals[:,0])\n",
    "                    sbp.append(vals[:,1])\n",
    "                    bt.append(vals[:,2])\n",
    "                    hr.append(vals[:,3])\n",
    "                    rr.append(vals[:,4])\n",
    "\n",
    "                    # extract non-null values\n",
    "                    dbp[i] = dbp[i][~np.isnan(dbp[i])]  \n",
    "                    sbp[i] = sbp[i][~np.isnan(sbp[i])] \n",
    "                    bt[i] = bt[i][~np.isnan(bt[i])]\n",
    "                    hr[i] = hr[i][~np.isnan(hr[i])] \n",
    "                    rr[i] = rr[i][~np.isnan(rr[i])]\n",
    "\n",
    "                except Exception as e: \n",
    "                    print('\\n=================\\n')\n",
    "                    print('INDEX: '+str(i))\n",
    "                    print('ERROR: '+str(type(e)))\n",
    "                    print('\\n=================\\n')\n",
    "                    pass\n",
    "\n",
    "        if(type in ('a','w')):\n",
    "            for i in range(0,n_cases):\n",
    "                #vals = vitaldb.load_case(caseids[i], ['ECG_II'], 0.01) #parameter 0.01 for a 'zoomed' ecg\n",
    "                vals = vitaldb.load_case(caseids[i], ['ECG_II'])\n",
    "                ecg.append(vals[:,0])\n",
    "    \n",
    "    return ecg,dbp,sbp,bt,hr,rr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_ills_API(type='n',n_cases=None):\n",
    "    caseids_all = vitaldb.find_cases(['ECG_II','ART_DBP','ART_SBP','BT','HR','RR']) # find ids of patient with this parameters\n",
    "    # Load dataset\n",
    "    df = pd.read_csv('https://api.vitaldb.net/cases')\n",
    "    df = df[df['asa'] > 3]\n",
    "\n",
    "    caseids_unhealthy = df['caseid'].to_numpy() \n",
    "    caseids = [el for el in caseids_all if el in caseids_unhealthy]\n",
    "\n",
    "    if(n_cases is None):\n",
    "        n_cases = len(caseids)\n",
    "\n",
    "    ecg = []\n",
    "    dbp = []\n",
    "    sbp = []\n",
    "    bt  = []\n",
    "    hr  = []\n",
    "    rr  = []\n",
    "\n",
    "    if(type in ('a','n','w')):\n",
    "\n",
    "        if(type in ('a','n')):\n",
    "\n",
    "            # load all the patients data \n",
    "            for i in range(0,n_cases): # Select only five patient for testing purpose; then len(caseids)\n",
    "                try:\n",
    "                    vals = vitaldb.load_case(caseids[i], ['ART_DBP','ART_SBP','BT','HR','RR'])\n",
    "                    dbp.append(vals[:,0])\n",
    "                    sbp.append(vals[:,1])\n",
    "                    bt.append(vals[:,2])\n",
    "                    hr.append(vals[:,3])\n",
    "                    rr.append(vals[:,4])\n",
    "\n",
    "                    # extract non-null values\n",
    "                    dbp[i] = dbp[i][~np.isnan(dbp[i])]  \n",
    "                    sbp[i] = sbp[i][~np.isnan(sbp[i])] \n",
    "                    bt[i] = bt[i][~np.isnan(bt[i])]\n",
    "                    hr[i] = hr[i][~np.isnan(hr[i])] \n",
    "                    rr[i] = rr[i][~np.isnan(rr[i])]\n",
    "\n",
    "                except Exception as e: \n",
    "                    print('\\n=================\\n')\n",
    "                    print('INDEX: '+str(i))\n",
    "                    print('ERROR: '+str(type(e)))\n",
    "                    print('\\n=================\\n')\n",
    "                    pass\n",
    "\n",
    "        if(type in ('a','w')):\n",
    "            for i in range(0,n_cases):\n",
    "                #vals = vitaldb.load_case(caseids[i], ['ECG_II'], 0.01) #parameter 0.01 for a 'zoomed' ecg\n",
    "                vals = vitaldb.load_case(caseids[i], ['ECG_II'])\n",
    "                ecg.append(vals[:,0])\n",
    "    \n",
    "    return ecg,dbp,sbp,bt,hr,rr\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disk data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_disk(path):\n",
    "    ecg = []\n",
    "    dbp = []\n",
    "    sbp = []\n",
    "    bt  = []\n",
    "    hr  = []\n",
    "    rr  = []\n",
    "\n",
    "# save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "    filepath = os.path.join(path,'numeric_data.vitaldb')\n",
    "    ecgpath = os.path.join(path,'ecg_data.vitaldb')\n",
    "    #ecgpath = '/Volumes/Windows/SIIA/ecg_data.vitaldb'\n",
    "\n",
    "    with open(ecgpath, 'rb') as f:\n",
    "        (ecg) = pickle.load(f)\n",
    "    with open(filepath, 'rb') as f:\n",
    "        (dbp,sbp,bt,hr,rr) = pickle.load(f)\n",
    "    \n",
    "    return ecg,dbp,sbp,bt,hr,rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_disk(path,ecg,dbp,sbp,bt,hr,rr):\n",
    "# save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "\n",
    "    with open(os.path.join(path,'numeric_data.vitaldb'), 'wb') as f:\n",
    "        pickle.dump((dbp,sbp,bt,hr,rr), f)\n",
    "\n",
    "    with open(os.path.join(path,'ecg_data.vitaldb'), 'wb') as f:\n",
    "        pickle.dump(ecg, f)\n",
    "\n",
    "\n",
    "\n",
    "## save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "#filepath = '/Users/Roberto/projects/siiaproject-vitanomaly/data.vitaldb'\n",
    "#ecgpath = '/Volumes/Windows/SIIA/ecg_data.vitaldb'\n",
    "#ecgpath = '/Users/Roberto/projects/siiaproject-vitanomaly/ecg_data.vitaldb'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(ecg,dbp,sbp,bt,hr,rr):\n",
    "    \n",
    "    p = np.argmax([len(el) for el in sbp])\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(511)\n",
    "    plt.title(\"DBP\")\n",
    "    plt.plot(dbp[p], color='b')\n",
    "    plt.subplots_adjust(hspace=1.)\n",
    "    plt.subplot(512)\n",
    "    plt.title(\"SBP\")\n",
    "    plt.plot(sbp[p][:], color='r')\n",
    "\n",
    "    plt.subplot(513)\n",
    "    plt.title(\"Body temperature\")\n",
    "    plt.plot(bt[p][:], color='orange')\n",
    "\n",
    "    plt.subplot(514)\n",
    "    plt.title(\"Heart rate\")\n",
    "    plt.plot(hr[p][:], color='r')\n",
    "\n",
    "    plt.subplot(515)\n",
    "    plt.title(\"Respiratory rate\")\n",
    "    plt.plot(rr[p][:], color='g')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(ecg,dbp,sbp,bt,hr,rr):\n",
    "# remove empty elements\n",
    "    try:\n",
    "        for i in range(0,len(dbp) - len([el for el in dbp if len(el) == 0])):\n",
    "            if(len(dbp[i])==0):\n",
    "                dbp.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(sbp) - len([el for el in sbp if len(el) == 0])):\n",
    "            if(len(sbp[i])==0):\n",
    "                sbp.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(bt) - len([el for el in bt if len(el) == 0])):\n",
    "            if(len(bt[i])==0):\n",
    "                bt.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(hr) - len([el for el in hr if len(el) == 0])):\n",
    "            if(len(hr[i])==0):\n",
    "                hr.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(rr) - len([el for el in rr if len(el) == 0])):\n",
    "            if(len(rr[i])==0):\n",
    "                rr.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(ecg) - len([el for el in ecg if len(el) == 0])):\n",
    "            if(len(ecg[i])==0):\n",
    "                ecg.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return ecg,dbp,sbp,bt,hr,rr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(ecg,dbp,sbp,bt,hr,rr):\n",
    "    old_settings = np.seterr(all='raise')\n",
    "    idx_remove = []\n",
    "    for i in range(0,len(ecg)): \n",
    "        try:\n",
    "            #Remove negative values\n",
    "            ecg[i][np.argwhere(ecg[i]<0)] = np.mean(ecg[i]) \n",
    "            # MinMax normalization\n",
    "            ecg[i] = (ecg[i] - np.min(ecg[i]))/(np.max(ecg[i])-np.min(ecg[i])) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove.append(i)\n",
    "\n",
    "    for idx in idx_remove: \n",
    "        ecg.pop(idx)\n",
    "\n",
    "    idx_remove = []\n",
    "    for i in range(0,len(dbp)): \n",
    "        try:\n",
    "            #Remove negative values\n",
    "            dbp[i][np.argwhere(dbp[i]<0)] = np.mean(dbp[i]) \n",
    "            # MinMax normalization\n",
    "            dbp[i] = (dbp[i] - np.min(dbp[i]))/(np.max(dbp[i])-np.min(dbp[i])) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove.append(i)\n",
    "\n",
    "    for idx in idx_remove: \n",
    "        dbp.pop(idx)\n",
    "\n",
    "    idx_remove = []  \n",
    "    for i in range(0,len(sbp)): \n",
    "        try:\n",
    "            #Remove negative values\n",
    "            sbp[i][np.argwhere(sbp[i]<0)] = np.mean(sbp[i]) \n",
    "            # MinMax normalization\n",
    "            sbp[i] = (sbp[i] - np.min(sbp[i]))/(np.max(sbp[i])-np.min(sbp[i])) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove.append(i)\n",
    "\n",
    "    for idx in idx_remove: \n",
    "        sbp.pop(idx)\n",
    "\n",
    "\n",
    "\n",
    "    idx_remove = []\n",
    "    for i in range(0,len(bt)): \n",
    "        try:\n",
    "            #Remove negative values\n",
    "            bt[i][np.argwhere(bt[i]<0)] = np.mean(bt[i]) \n",
    "            # MinMax normalization\n",
    "            bt[i] = (bt[i] - np.min(bt[i]))/(np.max(bt[i])-np.min(bt[i])) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove.append(i)\n",
    "\n",
    "    for idx in idx_remove: \n",
    "        bt.pop(idx)\n",
    "\n",
    "\n",
    "\n",
    "    idx_remove = []\n",
    "    for i in range(0,len(hr)): \n",
    "        try:\n",
    "            #Remove negative values\n",
    "            hr[i][np.argwhere(hr[i]<0)] = np.mean(hr[i]) \n",
    "            # MinMax normalization\n",
    "            hr[i] = (hr[i] - np.min(hr[i]))/(np.max(hr[i])-np.min(hr[i])) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove.append(i)\n",
    "\n",
    "    for idx in idx_remove: \n",
    "        hr.pop(idx)\n",
    "\n",
    "\n",
    "    idx_remove = []\n",
    "    for i in range(0,len(rr)): \n",
    "        try:\n",
    "            #Remove negative values\n",
    "            rr[i][np.argwhere(rr[i]<0)] = np.mean(rr[i]) \n",
    "            # MinMax normalization\n",
    "            rr[i] = (rr[i] - np.min(rr[i]))/(np.max(rr[i])-np.min(rr[i])) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove.append(i)\n",
    "\n",
    "    for idx in idx_remove: \n",
    "        rr.pop(idx)\n",
    "\n",
    "\n",
    "    # Back to default settings for errors\n",
    "    np.seterr(**old_settings)\n",
    "\n",
    "    return ecg,dbp,sbp,bt,hr,rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_data(path):\n",
    "    ecg,dbp,sbp,bt,hr,rr = load_from_disk(path=path)\n",
    "    ecg,dbp,sbp,bt,hr,rr = remove_empty(ecg,dbp,sbp,bt,hr,rr)\n",
    "    ecg,dbp,sbp,bt,hr,rr = normalize(ecg,dbp,sbp,bt,hr,rr )\n",
    "    return ecg,dbp,sbp,bt,hr,rr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg,dbp,sbp,bt,hr,rr = get_preprocessed_data('/Users/Roberto/projects/AnomalyDetection/data/raw')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = np.asarray([item for sublist in dbp for item in sublist],dtype='float64')\n",
    "print(flat_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = np.array_split(flat_list,1000)\n",
    "seq_len = len(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = int(len(chunks) * 0.7) # 70% percent of data fro training\n",
    "max_lenght = max([len(el) for el in chunks])\n",
    "# Uniform lenghts\n",
    "X_train = pad_sequences(chunks, max_lenght,padding='post',value=0.5,dtype='float64')\n",
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = X_train.shape[1]\n",
    "#input Layer\n",
    "input_layer = tf.keras.layers.Input(shape=(n_dim,))\n",
    "#Encoder\n",
    "encoder = tf.keras.layers.Dense(512, activation=\"tanh\", activity_regularizer=tf.keras.regularizers.l2(0.0005))(input_layer)\n",
    "encoder=tf.keras.layers.Dropout(0.2)(encoder)\n",
    "encoder = tf.keras.layers.Dense(256, activation='tanh')(encoder)\n",
    "encoder = tf.keras.layers.Dense(128, activation='relu')(encoder)\n",
    "encoder = tf.keras.layers.Dense(64, activation='relu')(encoder)\n",
    "# Decoder\n",
    "decoder = tf.keras.layers.Dense(64, activation='relu')(encoder)\n",
    "decoder = tf.keras.layers.Dense(128, activation='relu')(decoder)\n",
    "decoder = tf.keras.layers.Dense(256, activation='tanh')(decoder)\n",
    "decoder=tf.keras.layers.Dropout(0.2)(decoder)\n",
    "decoder = tf.keras.layers.Dense(512, activation='tanh')(decoder)\n",
    "decoder = tf.keras.layers.Dense(n_dim, activation='tanh')(decoder)\n",
    "#Autoencoder\n",
    "autoencoder = tf.keras.Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train,X_train,epochs=15,batch_size=64,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_p = vitaldb.load_case(263, ['ART_DBP','ART_SBP','BT','HR','RR'])\n",
    "test = val_p[:,0]\n",
    "\n",
    "test = test[~np.isnan(test)] \n",
    "\n",
    "print(np.shape(test))\n",
    "\n",
    "test = test[:seq_len]\n",
    "test = (test-np.min(test))/(np.max(test)-np.min(test))\n",
    "\n",
    "\n",
    "res = autoencoder.predict(np.expand_dims(test,axis=0))\n",
    "\n",
    "print(np.linalg.norm(test,2))\n",
    "print(np.linalg.norm(res[0],2))\n",
    "print(abs(np.linalg.norm(test,2)-np.linalg.norm(res[0],2)))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(211)\n",
    "plt.title('Original')\n",
    "plt.plot(test)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title('Reconstructed')\n",
    "\n",
    "plt.plot(res[0],scaley=1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a patient. NB: the data has to be normalized with MinMax approach\n",
    "#val_p = vitaldb.load_case(17, ['ART_DBP','ART_SBP','BT','HR','RR'])\n",
    "#\n",
    "## Pre-processing: remove nan vals, normalize and padding\n",
    "#val_p = val_p[~np.isnan(val_p)]\n",
    "#val_p = (val_p-np.min(val_p))/(np.max(val_p)-np.min(val_p))\n",
    "#test = pad_sequences([val_p],max_lenght,padding='post',value=0.5,dtype='float64')\n",
    "#\n",
    "#res = autoencoder.predict(test)\n",
    "#\n",
    "## The error for case 464 (ill patient) is much greater than case 17 (healthy patient)\n",
    "#print(np.linalg.norm(test,2))\n",
    "#print(np.linalg.norm(res[0],2))\n",
    "#print(abs(np.linalg.norm(test,2)-np.linalg.norm(res[0],2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Autoencoder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = np.asarray([item for sublist in dbp for item in sublist],dtype='float64')\n",
    "print(flat_list.shape)\n",
    "TIME_STEP = 6000 # 6000 is the mean length of the dbp time series\n",
    "X_train = []\n",
    "for seq in range(0,len(flat_list), TIME_STEP):\n",
    "    X_train.append(flat_list[seq:seq+TIME_STEP])\n",
    "X_train = np.asarray(X_train,dtype=object)\n",
    "X_train = pad_sequences(X_train, TIME_STEP,padding='post',value=0.5,dtype='float64')\n",
    "X_train = np.asarray(np.expand_dims(X_train,axis=2))\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, n_features= X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_len, n_features),return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(64,return_sequences=True,recurrent_activation='tanh'))\n",
    "model.add(LSTM(32,return_sequences=True,recurrent_activation='relu'))\n",
    "model.add(LSTM(32,return_sequences=True,recurrent_activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=True,recurrent_activation='tanh'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(128,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, X_train, epochs=10, batch_size=64, validation_split=0.1,shuffle=False,verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP = 6000 # 6000 is the mean length of the dbp time series\n",
    "\n",
    "flat_list_dbp = np.asarray([item for sublist in dbp for item in sublist],dtype='float64')\n",
    "flat_list_sbp = np.asarray([item for sublist in sbp for item in sublist],dtype='float64')\n",
    "flat_list_hr = np.asarray([item for sublist in hr for item in sublist],dtype='float64')\n",
    "flat_list_bt = np.asarray([item for sublist in bt  for item in sublist],dtype='float64')\n",
    "flat_list_rr = np.asarray([item for sublist in rr for item in sublist],dtype='float64')\n",
    "print({'dbp':len(flat_list_dbp),'sbp':len(flat_list_sbp),'hr':len(flat_list_hr),'bt':len(flat_list_bt),'rr':len(flat_list_rr)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dbp = []\n",
    "for seq in range(0,len(flat_list_dbp), TIME_STEP):\n",
    "    X_dbp.append(flat_list_dbp[seq:seq+TIME_STEP])\n",
    "X_dbp = np.asarray(X_dbp,dtype=object)\n",
    "X_dbp = pad_sequences(X_dbp, TIME_STEP,padding='post',value=0.5,dtype='float64')\n",
    "X_dbp = np.asarray(np.expand_dims(X_dbp,axis=2))\n",
    "\n",
    "\n",
    "X_sbp = []\n",
    "for seq in range(0,len(flat_list_sbp), TIME_STEP):\n",
    "    X_sbp.append(flat_list_sbp[seq:seq+TIME_STEP])\n",
    "X_sbp = np.asarray(X_sbp,dtype=object)\n",
    "X_sbp = pad_sequences(X_sbp, TIME_STEP,padding='post',value=0.5,dtype='float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sbp = X_sbp[:2945,:] # 2945 is the number of samples of dbp\n",
    "Y = np.concatenate([X_dbp, X_sbp[...,None]],axis=2)\n",
    "X_train = Y[:30,...] # Select the first 10 instances\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, n_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(seq_len, n_features),return_sequences=True))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(LSTM(32,return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, X_train, epochs=5,shuffle=False,verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,hdbp,hsbp,_,_,_ = load_ills_API(n_cases=5)\n",
    "#_,hdbp,hsbp,_,_,_ = load_healthy_API(n_cases=5)\n",
    "_,hdbp,hsbp,_,_,_ = remove_empty(_,hdbp,hsbp,_,_,_)\n",
    "_,hdbp,hsbp,_,_,_ = normalize(_,hdbp,hsbp,_,_,_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_hdbp = np.asarray([item for sublist in hdbp for item in sublist],dtype='float64')\n",
    "flat_list_hsbp = np.asarray([item for sublist in hsbp for item in sublist],dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP = 6000 # 6000 is the mean length of the dbp time series\n",
    "\n",
    "X_dbp = []\n",
    "for seq in range(0,len(flat_list_hdbp), TIME_STEP):\n",
    "    X_dbp.append(flat_list_hdbp[seq:seq+TIME_STEP])\n",
    "X_dbp = np.asarray(X_dbp,dtype=object)\n",
    "X_dbp = pad_sequences(X_dbp, TIME_STEP,padding='post',value=0.5,dtype='float64')\n",
    "X_dbp = np.asarray(np.expand_dims(X_dbp,axis=2))\n",
    "\n",
    "\n",
    "\n",
    "X_sbp = []\n",
    "for seq in range(0,len(flat_list_hsbp), TIME_STEP):\n",
    "    X_sbp.append(flat_list_hsbp[seq:seq+TIME_STEP])\n",
    "X_sbp = np.asarray(X_sbp,dtype=object)\n",
    "X_sbp = pad_sequences(X_sbp, TIME_STEP,padding='post',value=0.5,dtype='float64')\n",
    "\n",
    "\n",
    "\n",
    "X_sbp = X_sbp[:6,:] # 6 is the number of samples of dbp\n",
    "Y = np.concatenate([X_dbp, X_sbp[...,None]],axis=2)\n",
    "X_test = Y # Select the first n instances\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(np.linalg.norm(res[...,1]) - np.linalg.norm(X_test[...,1],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(211)\n",
    "plt.plot(res[...,1])\n",
    "plt.subplot(212)\n",
    "plt.plot(X_test[...,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "dim = 0\n",
    "abs(np.linalg.norm(res[p,:,dim]) - np.linalg.norm(X_test[p,:,dim],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(111)\n",
    "plt.plot(res[p,:,dim],'r',X_test[p,:,dim],'g')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83f3cce98b4a0628e00255681f870890cb90ecb6f8db2750deab685955efac63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
