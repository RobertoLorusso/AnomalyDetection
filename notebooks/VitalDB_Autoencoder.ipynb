{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DO/BUGS/Considerations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUGS\n",
    "\n",
    "- When introducing the ECG as feature, the prediction gives nan values. Need to try to implement a single feature LSTM autoencoder on the ECG and see if the behaviour is different\n",
    "\n",
    "- Pad sequences with the mean of the feature may force the autoencoder to learn the mean instead the original ditribution. This problem occurs with HR,RR and DBP. TIME_STEP has to be setted to minimum lenght of features sequences.\n",
    "\n",
    "\n",
    "### TO-DO\n",
    "\n",
    "- Describe methodology: holdout method, preprocessing, multivariate lstm autoencoder, loss function (mse), reconstruction error, time for training, mean reconstructino error approach and adaptive thresholding based on it. Experimental results.\n",
    "\n",
    "- 'if item > 0' condition when flattening inside normalization functions should be in clean_data function\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "- Should I use also the std of reconstruction error for threshold estimation, as described in the work Adaptive Threshold for Outlier Detection on Data Streams?\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- Nel caricare i dati la maggior parte dei sample era costituito da valori NaN. Una volta rimossi si ottengono dei valori con dei picchi, dovuti all'aver reso contigui valori che prima non lo erano.\n",
    "- Rimossi i record che hanno una lunghezza pari a zero o una media negativa.\n",
    "- L'errore di ricostruzione è minore usando la normalizzazione minmax, probabilmente perchè i dati non hanno una distribuzione gaussiana\n",
    "- TIME_STEP maggiori degradano le prestazioni"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vitaldb\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import operator\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, TimeDistributed, RepeatVector\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import regularizers\n",
    "from seaborn import histplot\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_from_API(n_cases=None, asa = 3 , op = '<'):\n",
    "\n",
    "    \"\"\" \n",
    "    This function loads the VitalDB data from APIs\n",
    "\n",
    "    INPUT\n",
    "\n",
    "    - n_cases: limit the number of samples to load\n",
    "    - asa: ASA status for health status classification, can range in [1,2,3,4,5,6]\n",
    "    - op: operator to use on asa status. Permits to load a specific portion of the dataset.\n",
    "    \n",
    "     \n",
    "    OUTPUT  \n",
    "\n",
    "    - dbp: diastolyc blood pressure samples\n",
    "    - sbp: systolic blood pressure samples\n",
    "    - bt: body temperature samples\n",
    "    - hr: heart rate samples\n",
    "    - rr: respiratory rate samples\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ops = {'>': operator.gt,\n",
    "       '<': operator.lt,\n",
    "       '>=': operator.ge,\n",
    "       '<=': operator.le,\n",
    "       '==': operator.eq}\n",
    "\n",
    "    caseids_all = vitaldb.find_cases(['ART_DBP','ART_SBP','BT','HR','RR']) # find ids of patient with this parameters\n",
    "    \n",
    "    df = pd.read_csv('https://api.vitaldb.net/cases') # Load dataset\n",
    "    df = df[ops[op](df['asa'], asa)] # ASA param identifies the health status of the patient\n",
    "\n",
    "    caseids_unhealthy = df['caseid'].to_numpy() \n",
    "    caseids = [el for el in caseids_all if el in caseids_unhealthy]\n",
    "\n",
    "    if(n_cases is None):\n",
    "        n_cases = len(caseids)\n",
    "\n",
    "    dbp = []\n",
    "    sbp = []\n",
    "    bt  = []\n",
    "    hr  = []\n",
    "    rr  = []\n",
    "\n",
    "     # load all the patients data \n",
    "    for i in range(0,n_cases): # Select only five patient for testing purpose; then len(caseids)\n",
    "        try:\n",
    "            vals = vitaldb.load_case(caseids[i], ['ART_DBP','ART_SBP','BT','HR','RR'])\n",
    "            dbp.append(vals[:,0])\n",
    "            sbp.append(vals[:,1])\n",
    "            bt.append(vals[:,2])\n",
    "            hr.append(vals[:,3])\n",
    "            rr.append(vals[:,4])\n",
    "            # extract non-null values\n",
    "            dbp[i] = dbp[i][~np.isnan(dbp[i])]  \n",
    "            sbp[i] = sbp[i][~np.isnan(sbp[i])] \n",
    "            bt[i] = bt[i][~np.isnan(bt[i])]\n",
    "            hr[i] = hr[i][~np.isnan(hr[i])] \n",
    "            rr[i] = rr[i][~np.isnan(rr[i])]\n",
    "        except Exception as e: \n",
    "            print('\\n=================\\n')\n",
    "            print('INDEX: '+str(i))\n",
    "            print('ERROR: '+str(type(e)))\n",
    "            print('\\n=================\\n')\n",
    "            pass\n",
    "\n",
    "    \n",
    "    return dbp,sbp,bt,hr,rr\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disk data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_disk(path):\n",
    "    \n",
    "    dbp = []\n",
    "    sbp = []\n",
    "    bt  = []\n",
    "    hr  = []\n",
    "    rr  = []\n",
    "\n",
    "# save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "    filepath = os.path.join(path,'numeric_data.vitaldb')\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        (dbp,sbp,bt,hr,rr) = pickle.load(f)\n",
    "    \n",
    "    return dbp,sbp,bt,hr,rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_disk(path,dbp,sbp,bt,hr,rr):\n",
    "# save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "\n",
    "    with open(os.path.join(path,'numeric_data.vitaldb'), 'wb') as f:\n",
    "        pickle.dump((dbp,sbp,bt,hr,rr), f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(dbp,sbp,bt,rr,hr):\n",
    "\n",
    "    \"\"\" \n",
    "    INPUT \n",
    "\n",
    "    - dbp: diastolyc blood pressure samples\n",
    "    - sbp: systolic blood pressure samples\n",
    "    - bt: body temperature samples\n",
    "    - hr: heart rate samples\n",
    "    - rr: respiratory rate samples\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # remove all the negative values since these vital parameters can only be positive\n",
    "    flat_list_dbp = np.asarray([item for sublist in dbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_sbp = np.asarray([item for sublist in sbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_hr = np.asarray([item for sublist in hr for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_bt = np.asarray([item for sublist in bt  for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_rr = np.asarray([item for sublist in rr for item in sublist if item > 0],dtype='float64')\n",
    "\n",
    "    d = {}\n",
    "    d['Dyastolic BP'] = flat_list_dbp[:]\n",
    "    d['Systolic BP'] = flat_list_sbp[:]\n",
    "    d['Heart rate'] = flat_list_hr[:]\n",
    "\n",
    "    b = {}\n",
    "    b['Body temperature'] = flat_list_bt[:]\n",
    "    b['Respiratory rate'] = flat_list_rr[:]\n",
    "\n",
    "    f,ax = plt.subplots(2,1,figsize=(10,10))\n",
    "    histplot(d,binwidth=1,ax=ax[0])\n",
    "    histplot(b,binwidth=1,ax=ax[1])\n",
    "\n",
    "\n",
    "\n",
    "def plot_data(dbp,sbp,bt,hr,rr,bins=10):\n",
    "    \"\"\" \n",
    "    INPUT \n",
    "\n",
    "    - dbp: diastolyc blood pressure samples\n",
    "    - sbp: systolic blood pressure samples\n",
    "    - bt: body temperature samples\n",
    "    - hr: heart rate samples\n",
    "    - rr: respiratory rate samples\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    flat_list_dbp = np.asarray([item for sublist in dbp for item in sublist if item > 0 ],dtype='float64')\n",
    "    flat_list_sbp = np.asarray([item for sublist in sbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_bt = np.asarray([item for sublist in bt for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_hr = np.asarray([item for sublist in hr for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_rr = np.asarray([item for sublist in rr for item in sublist if item > 0],dtype='float64')\n",
    "    \n",
    "\n",
    "    plt.subplots_adjust(hspace=1.)\n",
    "    plt.subplot(511)\n",
    "    plt.title(\"DBP\")\n",
    "    plt.hist(flat_list_dbp,bins=bins)\n",
    "\n",
    "    plt.subplots_adjust(hspace=1.)\n",
    "    plt.subplot(512)\n",
    "    plt.title(\"SBP\")\n",
    "    plt.hist(flat_list_sbp,bins=bins)\n",
    "\n",
    "    plt.subplot(513)\n",
    "    plt.title(\"Body temperature\")\n",
    "    plt.hist(flat_list_bt,bins=bins)\n",
    "\n",
    "    plt.subplot(514)\n",
    "    plt.title(\"Heart rate\")\n",
    "    plt.hist(flat_list_hr,bins=bins)\n",
    "\n",
    "    plt.subplot(515)\n",
    "    plt.title(\"Respiratory rate\")\n",
    "    plt.hist(flat_list_rr,bins=bins)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(dbp,sbp,bt,hr,rr):\n",
    "    \"\"\" \n",
    "    This function removes time series records which have either zero length (empty record) or negative mean value (affected by registration noise)\n",
    "\n",
    "    INPUT \n",
    "\n",
    "    - dbp: diastolyc blood pressure samples\n",
    "    - sbp: systolic blood pressure samples\n",
    "    - bt: body temperature samples\n",
    "    - hr: heart rate samples\n",
    "    - rr: respiratory rate samples\n",
    "\n",
    "    OUTPUT: cleaned data\n",
    "\n",
    "    NB: Blood pressure (dbp,sbp) ranges accordingly with the European guidelines ESC-ESH 2018.\n",
    "        Body temperature ranges in 35-37 \n",
    "        Hearth rate ranges in 60-100\n",
    "        Respiratory rate ranges in 10-21\n",
    "\n",
    "        These values for ranges are a bit wider than optimal ones since we want to preserve some variablity in the data. \n",
    "     \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(dbp)):\n",
    "            dbp[i] = dbp[i][dbp[i] > 50]\n",
    "            dbp[i] = dbp[i][dbp[i] < 90]  # we want to preserve some variability\n",
    "\n",
    "        for i in range(0,len(dbp) - len([el for el in dbp if len(el) == 0 or (np.mean(el) <= 0)])):\n",
    "            if(len(dbp[i])==0 or (np.mean(dbp[i]) <= 0)):\n",
    "                dbp.pop(i)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(sbp)):\n",
    "            sbp[i] = sbp[i][sbp[i] > 80]\n",
    "            sbp[i] = sbp[i][sbp[i] < 140] # we want to preserve some variability\n",
    "\n",
    "        for i in range(0,len(sbp) - len([el for el in sbp if len(el) == 0 or (np.mean(el) <= 0)])):\n",
    "            if(len(sbp[i])==0 or (np.mean(sbp[i]) <= 0)):\n",
    "                sbp.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(bt)):\n",
    "            bt[i] = bt[i][bt[i] > 35]\n",
    "            bt[i] = bt[i][bt[i] < 37.2]\n",
    "        for i in range(0,len(bt) - len([el for el in bt if len(el) == 0 or (np.mean(el) <= 0)])):\n",
    "            if(len(bt[i])==0 or (np.mean(bt[i]) <= 0)):\n",
    "                bt.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(hr)):\n",
    "            hr[i] = hr[i][hr[i] > 60]\n",
    "            hr[i] = hr[i][hr[i] < 100] # we want to preserve some variability\n",
    "\n",
    "        for i in range(0,len(hr) - len([el for el in hr if len(el) == 0 or (np.mean(el) <= 0)])):\n",
    "            if(len(hr[i])==0 or (np.mean(hr[i]) <= 0)):\n",
    "                hr.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(rr)):\n",
    "            rr[i] = rr[i][rr[i] > 10]\n",
    "            rr[i] = rr[i][rr[i] < 21] # we want to preserve some variability\n",
    "\n",
    "        for i in range(0,len(rr) - len([el for el in rr if len(el) == 0 or (np.mean(el) <= 0)])):\n",
    "            if(len(rr[i])==0 or (np.mean(rr[i]) <= 0)):\n",
    "                rr.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return dbp,sbp,bt,hr,rr\n",
    "\n",
    "\n",
    "def mean_norm(dbp,sbp,bt,hr,rr):\n",
    "\n",
    "    # remove all the negative values since these vital parameters can only be positive\n",
    "    flat_list_dbp = np.asarray([item for sublist in dbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_sbp = np.asarray([item for sublist in sbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_hr = np.asarray([item for sublist in hr for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_bt = np.asarray([item for sublist in bt  for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_rr = np.asarray([item for sublist in rr for item in sublist if item > 0],dtype='float64')\n",
    "    print({'dbp':len(dbp),'sbp':len(sbp),'hr':len(hr),'bt':len(bt),'rr':len(rr)})\n",
    "\n",
    "\n",
    "\n",
    "    dbp_mean,dbp_std = np.mean(flat_list_dbp),np.std(flat_list_dbp)\n",
    "    sbp_mean,sbp_std = np.mean(flat_list_sbp),np.std(flat_list_sbp)\n",
    "    bt_mean,bt_std = np.mean(flat_list_bt),np.std(flat_list_bt)\n",
    "    hr_mean,hr_std = np.mean(flat_list_hr),np.std(flat_list_hr)\n",
    "    rr_mean,rr_std = np.mean(flat_list_rr),np.std(flat_list_rr)\n",
    "\n",
    "    print('\\nMean values for features:')\n",
    "    print({'dbp':dbp_mean,'sbp':sbp_mean,'hr':hr_mean,'bt':bt_mean,'rr':rr_mean})\n",
    "    print('\\nStd values for features:')\n",
    "    print({'dbp':dbp_std,'sbp':sbp_std,'hr':hr_std,'bt':bt_std,'rr':rr_std})\n",
    "    print('\\n')\n",
    "\n",
    "    # Consider runtime warnings such as Divide by zero as Exceptions to throw\n",
    "    old_settings = np.seterr(divide='raise')\n",
    "    idx_remove = {}\n",
    "    idx_remove['sbp'] = []\n",
    "    idx_remove['dbp'] = []\n",
    "    idx_remove['bt'] = []\n",
    "    idx_remove['rr'] = []\n",
    "    idx_remove['hr'] = []\n",
    "    \n",
    "\n",
    "    for i in range(0,len(dbp)): \n",
    "        try:\n",
    "            dbp[i] = (dbp[i] - dbp_mean)/(dbp_std) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['dbp'].append(i)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(0,len(sbp)): \n",
    "        try:\n",
    "            sbp[i] = (sbp[i] - sbp_mean)/(sbp_std) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['sbp'].append(i)\n",
    "        \n",
    "\n",
    "\n",
    "    for i in range(0,len(bt)): \n",
    "        try:\n",
    "            bt[i] = (bt[i] - bt_mean)/(bt_std) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['bt'].append(i)\n",
    "\n",
    "\n",
    "    for i in range(0,len(hr)): \n",
    "        try:\n",
    "            hr[i] = (hr[i] - hr_mean)/(hr_std) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['hr'].append(i)\n",
    "\n",
    "    for i in range(0,len(rr)): \n",
    "        try:\n",
    "            rr[i] = (rr[i] - rr_mean)/(rr_std) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['hr'].append(i)\n",
    "\n",
    "    # Back to default settings for errors\n",
    "    np.seterr(**old_settings)\n",
    "\n",
    "    return dbp,sbp,bt,hr,rr,idx_remove\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def minmax_norm(dbp,sbp,bt,hr,rr):\n",
    "\n",
    "\n",
    "    # remove all the negative values since these vital parameters can only be positive\n",
    "    flat_list_dbp = np.asarray([item for sublist in dbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_sbp = np.asarray([item for sublist in sbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_hr = np.asarray([item for sublist in hr for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_bt = np.asarray([item for sublist in bt  for item in sublist if item > 0] ,dtype='float64')\n",
    "    flat_list_rr = np.asarray([item for sublist in rr for item in sublist if item > 0],dtype='float64')\n",
    "    #print({'dbp':len(flat_list_dbp),'sbp':len(flat_list_sbp),'hr':len(flat_list_hr),'bt':len(flat_list_bt),'rr':len(flat_list_rr)})\n",
    "\n",
    "\n",
    "    dbp_min,dbp_max = np.min(flat_list_dbp),np.max(flat_list_dbp)\n",
    "    sbp_min,sbp_max = np.min(flat_list_sbp),np.max(flat_list_sbp)\n",
    "    bt_min,bt_max = np.min(flat_list_bt),np.max(flat_list_bt)\n",
    "    hr_min,hr_max = np.min(flat_list_hr),np.max(flat_list_hr)\n",
    "    rr_min,rr_max = np.min(flat_list_rr),np.max(flat_list_rr)\n",
    "    \n",
    "    old_settings = np.seterr(divide='raise')\n",
    "    idx_remove = {}\n",
    "\n",
    "    idx_remove['sbp'] = []\n",
    "    idx_remove['dbp'] = []\n",
    "    idx_remove['bt'] = []\n",
    "    idx_remove['rr'] = []\n",
    "    idx_remove['hr'] = []\n",
    "\n",
    "    for i in range(0,len(dbp)): \n",
    "        try:\n",
    "            dbp[i] = (dbp[i] - dbp_min)/(dbp_max - dbp_min) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['dbp'].append(i)\n",
    "\n",
    "\n",
    "    for i in range(0,len(sbp)): \n",
    "        try:\n",
    "            sbp[i] = (sbp[i] - sbp_min)/(sbp_max - sbp_min) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['sbp'].append(i)\n",
    "\n",
    "\n",
    "    for i in range(0,len(bt)): \n",
    "        try:\n",
    "            bt[i] = (bt[i] - bt_min)/(bt_max - bt_min) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['bt'].append(i)\n",
    "\n",
    "\n",
    "    for i in range(0,len(hr)): \n",
    "        try:\n",
    "            hr[i] = (hr[i] - hr_min)/(hr_max - hr_min) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['hr'].append(i)\n",
    "\n",
    "\n",
    "    for i in range(0,len(rr)): \n",
    "        try:\n",
    "            rr[i] = (rr[i] - rr_min)/(rr_max - rr_min) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['rr'].append(i)\n",
    "\n",
    "\n",
    "    # Back to default settings for errors\n",
    "    np.seterr(**old_settings)\n",
    "\n",
    "    return dbp,sbp,bt,hr,rr,idx_remove\n",
    "\n",
    "\n",
    "\n",
    "def get_preprocessed_data(path, raw = False, norm_func = None):\n",
    "\n",
    "    dbp,sbp,bt,hr,rr = load_from_disk(path=path)\n",
    "    \n",
    "    if(not raw):\n",
    "        dbp,sbp,bt,hr,rr = clean_data(dbp,sbp,bt,hr,rr)\n",
    "\n",
    "    idx_remove = []\n",
    "\n",
    "    if(norm_func is not None):\n",
    "        dbp,sbp,bt,hr,rr,idx_remove = norm_func(dbp,sbp,bt,hr,rr)\n",
    "        \n",
    "    return np.asarray(dbp),np.asarray(sbp),np.asarray(bt),np.asarray(hr),np.asarray(rr),idx_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/Roberto/projects/AnomalyDetection/data/processed'\n",
    "norm = 'mean'\n",
    "try:\n",
    "    func_norm = norm + '_norm'\n",
    "    normalize = globals()[func_norm] # normalize is called later for test data normalization\n",
    "except:\n",
    "    normalize = None\n",
    "\n",
    "dbp,sbp,bt,hr,rr,idx_remove = get_preprocessed_data(path = path,norm_func = normalize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declarations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME_STEP of preprocess function should be equal to the minimum length of features sequences\n",
    "min_seq_len = np.min([np.mean([len(el) for el in dbp]),np.mean([len(el) for el in sbp]),np.mean([len(el) for el in bt]),np.mean([len(el) for el in rr]),np.mean([len(el) for el in hr])])\n",
    "print('Mean length of sequences between features')\n",
    "min_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dbp,sbp,bt,hr,rr, TIME_STEP = 5500, n_instances = None):\n",
    "\n",
    "    \"\"\"\n",
    "    This function prepare the data to be input to the LSTM autoencoder, both for training or testing purpose.\n",
    "    This function assumes that the input data is already normalized, thus NO values modifications are needed when flattening.\n",
    "\n",
    "    INPUT \n",
    "\n",
    "    - dbp: diastolyc blood pressure samples\n",
    "    - sbp: systolic blood pressure samples\n",
    "    - bt: body temperature samples\n",
    "    - hr: heart rate samples\n",
    "    - rr: respiratory rate samples\n",
    "\n",
    "    - TIME_STEP: length of sequences for lstm autoencoder. Default to 5500 since it's the mean length of features sequences\n",
    "    - n_instances: number of instances to retain. Useful for testing purposes i.e. training the data on a smaller portion of data to computational resources and testing time.\n",
    "\n",
    "    OUTPUT\n",
    "\n",
    "    - X: tensor to be input to the LSTM autoencoder\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # flat the data to be later chuncked in sequences\n",
    "\n",
    "    flat_list_dbp = np.asarray([item for sublist in dbp for item in sublist],dtype='float64')\n",
    "    flat_list_sbp = np.asarray([item for sublist in sbp for item in sublist],dtype='float64')\n",
    "    flat_list_bt = np.asarray([item for sublist in bt for item in sublist],dtype='float64')\n",
    "    flat_list_hr = np.asarray([item for sublist in hr for item in sublist],dtype='float64')\n",
    "    flat_list_rr = np.asarray([item for sublist in rr for item in sublist],dtype='float64')\n",
    "\n",
    "\n",
    "\n",
    "    X_dbp = []\n",
    "    X_sbp = []\n",
    "    X_bt = []\n",
    "    X_hr = []\n",
    "    X_rr = []\n",
    "\n",
    "    for seq in range(0,len(flat_list_dbp), TIME_STEP):\n",
    "        X_dbp.append(flat_list_dbp[seq:seq+TIME_STEP])\n",
    "\n",
    "    for seq in range(0,len(flat_list_sbp), TIME_STEP):\n",
    "        X_sbp.append(flat_list_sbp[seq:seq+TIME_STEP])\n",
    "\n",
    "    for seq in range(0,len(flat_list_bt), TIME_STEP):\n",
    "        X_bt.append(flat_list_bt[seq:seq+TIME_STEP])\n",
    "\n",
    "    for seq in range(0,len(flat_list_hr), TIME_STEP):\n",
    "        X_hr.append(flat_list_hr[seq:seq+TIME_STEP])\n",
    "\n",
    "    for seq in range(0,len(flat_list_rr), TIME_STEP):\n",
    "        X_rr.append(flat_list_rr[seq:seq+TIME_STEP])\n",
    "\n",
    "\n",
    "    X_dbp = np.asarray(X_dbp,dtype=object)\n",
    "    X_sbp = np.asarray(X_sbp,dtype=object)\n",
    "    X_bt = np.asarray(X_bt,dtype=object)\n",
    "    X_hr = np.asarray(X_hr,dtype=object)\n",
    "    X_rr = np.asarray(X_rr,dtype=object)\n",
    "\n",
    "    # Pad with mean value of data\n",
    "    X_dbp = pad_sequences(X_dbp, TIME_STEP,padding='post',value = np.mean(flat_list_dbp),dtype='float64')\n",
    "    X_sbp = pad_sequences(X_sbp, TIME_STEP,padding='post',value = np.mean(flat_list_sbp),dtype='float64')\n",
    "    X_bt = pad_sequences(X_bt, TIME_STEP,padding='post',value = np.mean(flat_list_bt),dtype='float64')\n",
    "    X_hr = pad_sequences(X_hr, TIME_STEP,padding='post',value = np.mean(flat_list_hr),dtype='float64')\n",
    "    X_rr = pad_sequences(X_rr, TIME_STEP,padding='post',value = np.mean(flat_list_rr),dtype='float64')\n",
    "\n",
    "\n",
    "\n",
    "    X_dbp = np.asarray(np.expand_dims(X_dbp,axis=2))\n",
    "    X_sbp = np.asarray(np.expand_dims(X_sbp,axis=2))\n",
    "    X_bt = np.asarray(np.expand_dims(X_bt,axis=2))\n",
    "    X_hr = np.asarray(np.expand_dims(X_hr,axis=2))\n",
    "    X_rr = np.asarray(np.expand_dims(X_rr,axis=2))\n",
    "\n",
    "    # Truncate on the minimum length of features sequences\n",
    "    min_len = np.min([X_dbp.shape[0],X_sbp.shape[0],X_bt.shape[0],X_hr.shape[0],X_rr.shape[0]])\n",
    "    \n",
    "    X_dbp = X_dbp[:min_len,:]\n",
    "    X_sbp = X_sbp[:min_len,:]\n",
    "    X_bt = X_bt[:min_len,:]\n",
    "    X_hr = X_hr[:min_len,:]\n",
    "    X_rr = X_rr[:min_len,:]\n",
    "\n",
    "    Y = np.concatenate([X_dbp, X_sbp, X_bt, X_hr, X_rr],axis=2)\n",
    "    \n",
    "    if(isinstance(n_instances,int) and n_instances > 0):\n",
    "        X = Y[:n_instances] # Select the first n instances\n",
    "    else:\n",
    "        X = Y\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def eval_error(X_test,res,std=False):\n",
    "\n",
    "    \"\"\"\n",
    "    This function evaluates the reconstruction error of the input samples, \n",
    "    returning the mean reconstruction error for every feature and for the overall samples.\n",
    "\n",
    "    INPUT\n",
    "\n",
    "    - X_test : test samples\n",
    "    - res_test: reconstructed test samples\n",
    "    - std: Boolean, return standard devation for reconstruction error of the features\n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    - overall_mean: mean reconstruction error for the entire input data\n",
    "    - dict_means: Dictionary, contains the mean reconstruction error for the single features\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n = min(X_test.shape[0],res.shape[0])\n",
    "    n_features = min(X_test.shape[2],res.shape[2])\n",
    "\n",
    "    overall_mean = 0\n",
    "\n",
    "    dict_means = {}\n",
    "    dict_means['sbp'] = 0\n",
    "    dict_means['dbp'] = 0\n",
    "    dict_means['bt'] = 0\n",
    "    dict_means['rr'] = 0\n",
    "    dict_means['hr'] = 0\n",
    "\n",
    "    dic_means = {}\n",
    "    dic_means[0] = 0\n",
    "    dic_means[1] = 0\n",
    "    dic_means[2] = 0\n",
    "    dic_means[3] = 0\n",
    "    dic_means[4] = 0\n",
    "\n",
    "    if(std):\n",
    "        dict_std = {}\n",
    "        dict_std['sbp'] = 0\n",
    "        dict_std['dbp'] = 0\n",
    "        dict_std['bt'] = 0\n",
    "        dict_std['rr'] = 0\n",
    "        dict_std['hr'] = 0\n",
    "\n",
    "        dic_std = {}\n",
    "        dic_std[0] = 0\n",
    "        dic_std[1] = 0\n",
    "        dic_std[2] = 0\n",
    "        dic_std[3] = 0\n",
    "        dic_std[4] = 0\n",
    "    \n",
    "\n",
    "    for i in range(0,n_features):\n",
    "\n",
    "        tot = 0\n",
    "        for p in range(0,n):\n",
    "            re = np.linalg.norm(X_test[p,:,i] - res[p,:,i],2)\n",
    "            tot = tot + re            \n",
    "            dic_means[i] = dic_means[i] + re\n",
    " \n",
    "        mean = np.round(tot/n,3)\n",
    "        overall_mean = overall_mean + mean\n",
    "        \n",
    "        if(std):\n",
    "            # compute the variance\n",
    "            for p in range(0,n):\n",
    "                re = np.linalg.norm(X_test[p,:,i] - res[p,:,i],2)\n",
    "                dic_std[i] = (re - mean)**2 + dic_std[i]\n",
    "\n",
    "\n",
    "\n",
    "    dict_means['sbp'] = np.round(dic_means[0]/n,4)\n",
    "    dict_means['dbp'] = np.round(dic_means[1]/n,4)\n",
    "    dict_means['bt']  = np.round(dic_means[2]/n,4)\n",
    "    dict_means['rr']  = np.round(dic_means[3]/n,4)\n",
    "    dict_means['hr']  = np.round(dic_means[4]/n,4)\n",
    "\n",
    "    if(std):\n",
    "        dict_std['sbp'] = np.round((dic_std[0]/n)**(1/2),4)\n",
    "        dict_std['dbp'] = np.round((dic_std[1]/n)**(1/2),4)\n",
    "        dict_std['bt']  = np.round((dic_std[2]/n)**(1/2),4)\n",
    "        dict_std['rr']  = np.round((dic_std[3]/n)**(1/2),4)\n",
    "        dict_std['hr']  = np.round((dic_std[4]/n)**(1/2),4)\n",
    "        del dic_std\n",
    "\n",
    "    del dic_means\n",
    "    \n",
    "\n",
    "    overall_mean = overall_mean/n_features\n",
    "\n",
    "    if(std):\n",
    "        return overall_mean,dict_means,dict_std\n",
    "    else:\n",
    "        return overall_mean,dict_means\n",
    "\n",
    "\n",
    "\n",
    "def select_threshold(X, res, mean_rec_err, perc=.98, nit=10000, step=None, out=False):\n",
    "\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "\n",
    "    - X : training data of the autoencoder\n",
    "    - res: reconstructed data from autoencoder\n",
    "    - mean_rec_err:  mean reconstruction error obtained on training data\n",
    "    - perc: percentage of training data for which the reconstruction error must be below the mean_rec_err multiplied by a factor\n",
    "    - nit: max number of iterations\n",
    "    - step: step with which increase the multiplication factor at every step\n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    - threshold: threshold for reconstruction error on test data, for which considering a sample anomalous\n",
    "\n",
    "    NB: An higher percentage value will increase the threshold, resulting in a looser anomaly detection. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    counter = 0\n",
    "    target = 0\n",
    "    dim = X.shape[0]\n",
    "    if(step is None):\n",
    "        step = 1 - perc # step estimation from perc for a better accuracy in factor estimation\n",
    "    factor = 1 - step # This subtraction is needed for the factor to be equal to one at the first step of while cycle\n",
    "    \n",
    "    while( target < perc and counter < nit):\n",
    "        j = 0\n",
    "        counter = counter + 1\n",
    "        factor = factor + step\n",
    "\n",
    "        for n in range(dim):\n",
    "            err = np.linalg.norm(X[n,...]-res[n,...],2)\n",
    "            if(abs(err <= mean_rec_err * factor)):\n",
    "                j = j+1\n",
    "        target = np.round(j/dim,2)\n",
    "        if(out):\n",
    "            print(target,factor)\n",
    "    \n",
    "    threshold = np.round(factor * mean_rec_err,3)\n",
    "    return threshold\n",
    "\n",
    "\n",
    "\n",
    "def select_features_threshold(X, res, dict_means, perc=.98, nit=10000, step=None, out=False):\n",
    "\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "\n",
    "    - X : training data of the autoencoder\n",
    "    - res: reconstructed data from autoencoder\n",
    "    - mean_rec_err:  mean reconstruction error obtained on training data\n",
    "    - perc: percentage of training data for which the reconstruction error must be below the mean_rec_err multiplied by a factor\n",
    "    - nit: max number of iterations\n",
    "    - step: step with which increase the multiplication factor at every step\n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    - thresholds: threshold for reconstruction error on test data, for which considering a sample anomalous\n",
    "\n",
    "    Note: An anomaly will be a sample that differs from the percentage of data specified.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    dim = X.shape[0]\n",
    "    n_features = X.shape[2]\n",
    "    means_rec_err = [dict_means[k] for k in dict_means.keys()]\n",
    "    factors = np.zeros(len(means_rec_err))\n",
    "    thresholds = np.zeros(len(means_rec_err))\n",
    "\n",
    "    if(step is None):\n",
    "        step = 1 - perc # step estimation from perc for a better accuracy in factor estimation\n",
    "    factors[:] = 1 - step\n",
    "    \n",
    "    for f in range(0,n_features):\n",
    "\n",
    "        counter = 0 # iterations counter\n",
    "        target = 0\n",
    "        if(out):\n",
    "            print('\\n\\t\\t\\tFEATURE ' + str(f))\n",
    "        while( target < perc and counter < nit):\n",
    "\n",
    "            j = 0 # numerator of percentage estimation j/dim\n",
    "            counter = counter + 1\n",
    "            factors[f] = factors[f] + step\n",
    "\n",
    "            for n in range(dim):\n",
    "                err = np.linalg.norm(X[n,:,f]-res[n,:,f],2)\n",
    "                if(abs(err <= means_rec_err[f]*factors[f])):\n",
    "                    j = j+1\n",
    "\n",
    "            target = np.round(j/dim,2)\n",
    "            if(out):\n",
    "                print(target,factors[f])\n",
    "\n",
    "        thresholds[f] = np.round(factors[f]*means_rec_err[f],3)\n",
    "\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "\n",
    "def detect_anomalies(X_test,res_test,threshold,out=True):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "\n",
    "    - X_test : test samples\n",
    "    - res_test: reconstructed test samples\n",
    "    - threshold: threshold for reconstruction error on test data, for which considering a sample anomalous. See select_threshold for details\n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    - idx_anomalies: list of indices of test samples deemed anomalous on the basis of the given threshold.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dim = X_test.shape[0]\n",
    "    idx_anomalies = []\n",
    "\n",
    "    for i in range(dim):\n",
    "        err = np.linalg.norm(X_test[i,...]-res_test[i,...],2) \n",
    "        if(abs(err > threshold)):\n",
    "            idx_anomalies.append(i)\n",
    "    \n",
    "    if(out):\n",
    "        print('\\nPercentage of anomalies in tested data:' + str(np.round(len(idx_anomalies)/X_test.shape[0],2)))\n",
    "\n",
    "    return idx_anomalies\n",
    "\n",
    "\n",
    "\n",
    "def detect_features_anomalies(X_test,res_test,thresholds,out=False):\n",
    "\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "\n",
    "    - X_test : test samples\n",
    "    - res_test: reconstructed test samples\n",
    "    - threshold: threshold for reconstruction error on test data, for which considering a sample anomalous. See select_threshold for details\n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    - idx_anomalies: list of indices of test samples deemed anomalous on the basis of the given threshold.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dim,n_features = X_test.shape[0],X_test.shape[2]\n",
    "    idx_anomalies = {}\n",
    "    idx_anomalies['sbp'] = []\n",
    "    idx_anomalies['dbp'] = []\n",
    "    idx_anomalies['bt'] = []\n",
    "    idx_anomalies['rr'] = []\n",
    "    idx_anomalies['hr'] = []\n",
    "\n",
    "    dic = {}\n",
    "    dic[0] = []\n",
    "    dic[1] = []\n",
    "    dic[2] = []\n",
    "    dic[3] = []\n",
    "    dic[4] = []\n",
    "\n",
    "    for f in range(0,n_features):\n",
    "        for i in range(dim):\n",
    "            err = np.linalg.norm(X_test[i,:,f] - res_test[i,:,f],2) \n",
    "            if(abs(err > thresholds[f])):\n",
    "                dic[f].append(i)\n",
    "\n",
    "    idx_anomalies['sbp'] = dic[0]\n",
    "    idx_anomalies['dbp'] = dic[1]\n",
    "    idx_anomalies['bt'] = dic[2]\n",
    "    idx_anomalies['rr'] = dic[3]\n",
    "    idx_anomalies['hr'] = dic[4]\n",
    "\n",
    "    if(out):\n",
    "        print('\\nPercentage of anomalies in tested data:')\n",
    "        for i in range(0,n_features):\n",
    "            print('\\nFEATURE ' + str(i) + ': ' + str(np.round(len(dic[i])/X_test.shape[0],2)))\n",
    "\n",
    "    del dic\n",
    "    return idx_anomalies\n",
    "\n",
    "\n",
    "def plot_predict(X_test, res, figsize=(40,40), n = None, single = False, vals = None):\n",
    "\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "\n",
    "    - X_test : data samples to plot\n",
    "    - res_test: reconstructed data samples to plot\n",
    "    - figsize: size of the plot\n",
    "    - n: number of samples to plot\n",
    "    - single: Boolean, when True plots the single instance number specified by 'n'\n",
    "    - vals: List of indices of samples to plot\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    if(n is not None and isinstance(n,int) and n > -1):\n",
    "        if(single):\n",
    "            for j in range(0,5):\n",
    "                s = '61'+str(j+1)\n",
    "                plt.subplot(int(s))\n",
    "                plt.subplots_adjust(hspace=1.)\n",
    "                plt.title('Feature ' + str(j+1))\n",
    "                plt.plot(res[n,:,j],'-r',linewidth=2.5,)\n",
    "                plt.plot(X_test[n,:,j],'-g',linewidth=2.5,alpha=0.8)\n",
    "            plt.show()\n",
    "        else:\n",
    "            for j in range(0,5):\n",
    "                for i in range(0,n):\n",
    "                    s = '61'+str(j+1)\n",
    "                    plt.subplot(int(s))\n",
    "                    plt.subplots_adjust(hspace=1.)\n",
    "                    plt.title('Feature ' + str(j+1))\n",
    "                    plt.plot(res[i,:,j],'-r',linewidth=2.5,)\n",
    "                    plt.plot(X_test[i,:,j],'-g',linewidth=2.,alpha=0.5)\n",
    "            plt.show()\n",
    "    elif(vals is not None):\n",
    "        for j in range(0,5):\n",
    "            for i in vals:\n",
    "                s = '61'+str(j+1)\n",
    "                plt.subplot(int(s))\n",
    "                plt.subplots_adjust(hspace=1.)\n",
    "                plt.title('Feature ' + str(j+1))\n",
    "                plt.plot(res[i,:,j],'-r',linewidth=2.5,)\n",
    "                plt.plot(X_test[i,:,j],'-g',linewidth=2.,alpha=0.5)\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        n = X_test.shape[0]\n",
    "        for j in range(0,5):\n",
    "            for i in range(0,n):\n",
    "                s = '61'+str(j+1)\n",
    "                plt.subplot(int(s))\n",
    "                plt.subplots_adjust(hspace=1.)\n",
    "                plt.title('Feature ' + str(j+1))\n",
    "                plt.plot(res[i,:,j],'-r',linewidth=2.5,)\n",
    "                plt.plot(X_test[i,:,j],'-g',linewidth=2.,alpha=0.5)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess(dbp,sbp,bt,hr,rr)\n",
    "batch_size, seq_len, n_features = X_train.shape\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, bias_regularizer=regularizers.l2(0.7),  recurrent_regularizer=regularizers.l2(0.6) , input_shape=(seq_len, n_features),return_sequences=True))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(LSTM(128,    bias_regularizer=regularizers.l2(0.7),  recurrent_regularizer=regularizers.l2(0.6),   return_sequences=True))\n",
    "model.add(LSTM(64,  bias_regularizer=regularizers.l1(0.7), recurrent_regularizer=regularizers.l2(0.6),  return_sequences=True))\n",
    "model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "model.add(LSTM(128,    bias_regularizer=regularizers.l2(0.7),  recurrent_regularizer=regularizers.l2(0.6),   return_sequences=True))\n",
    "model.add(LSTM(256,   bias_regularizer=regularizers.l2(0.7), recurrent_regularizer=regularizers.l2(0.6),  return_sequences=True))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model\n",
    "\n",
    "Added RepeatVector layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess(dbp,sbp,bt,hr,rr)\n",
    "batch_size, seq_len, n_features = X_train.shape\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, bias_regularizer=regularizers.l2(0.7),  recurrent_regularizer=regularizers.l2(0.6) , input_shape=(seq_len, n_features),return_sequences=True))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(LSTM(128,    bias_regularizer=regularizers.l2(0.7),  recurrent_regularizer=regularizers.l2(0.6),   return_sequences=True))\n",
    "model.add(LSTM(64,  bias_regularizer=regularizers.l1(0.7), recurrent_regularizer=regularizers.l2(0.6),  return_sequences=False))\n",
    "model.add(RepeatVector(seq_len))\n",
    "model.add(LSTM(64,  bias_regularizer=regularizers.l1(0.7), recurrent_regularizer=regularizers.l2(0.6),  return_sequences=True))\n",
    "model.add(LSTM(128,    bias_regularizer=regularizers.l2(0.7),  recurrent_regularizer=regularizers.l2(0.6),   return_sequences=True))\n",
    "model.add(LSTM(256,   bias_regularizer=regularizers.l2(0.7), recurrent_regularizer=regularizers.l2(0.6),  return_sequences=True))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, save and load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_first = tf.keras.models.load_model(\"/Users/Roberto/projects/AnomalyDetection/models/first/autoencoder_LSTM_v2\")\n",
    "#hist = model.fit(X_train,X_train, batch_size = 64, validation_split=0.1, verbose=1)\n",
    "#model.save(\"/Users/Roberto/projects/AnomalyDetection/models/first/autoencoder_LSTM_v3.h5\")\n",
    "#model.save(\"/Users/Roberto/projects/AnomalyDetection/models/first/autoencoder_LSTM_v3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_second = tf.keras.models.load_model(\"/Users/Roberto/projects/AnomalyDetection/models/second/autoencoder_LSTM_v1\")\n",
    "#model_second.fit(X_train,X_train, batch_size = 64, validation_split=0.1, verbose=1)\n",
    "#model_second.save(\"/Users/Roberto/projects/AnomalyDetection/models/second/autoencoder_LSTM_v1.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: if data il loaded from data/raw or from API need to call clean_data() and normalize()\n",
    "path = '/Users/Roberto/projects/AnomalyDetection/data/processed/'\n",
    "with open(os.path.join(path,'numeric_data_ills.vitaldb'), 'rb') as f:\n",
    "    (dbp_asa_gt_3_,sbp_asa_gt_3_,bt_asa_gt_3_,hr_asa_gt_3_,rr_asa_gt_3_ )  = pickle.load(f)\n",
    "\n",
    "dbp_asa_gt_3,sbp_asa_gt_3,bt_asa_gt_3,hr_asa_gt_3,rr_asa_gt_3 =  (dbp_asa_gt_3_,sbp_asa_gt_3_,bt_asa_gt_3_,hr_asa_gt_3_,rr_asa_gt_3_ )\n",
    "\n",
    "dbp_asa_eq_3,sbp_asa_eq_3,bt_asa_eq_3,hr_asa_eq_3,rr_asa_eq_3   = load_from_API(asa= 3, op='==', n_cases=100)\n",
    "dbp_asa_eq_3,sbp_asa_eq_3,bt_asa_eq_3,hr_asa_eq_3,rr_asa_eq_3   = clean_data(dbp_asa_eq_3,sbp_asa_eq_3,bt_asa_eq_3,hr_asa_eq_3,rr_asa_eq_3 )\n",
    "dbp_asa_eq_3,sbp_asa_eq_3,bt_asa_eq_3,hr_asa_eq_3,rr_asa_eq_3,_ =  normalize(dbp_asa_eq_3,sbp_asa_eq_3,bt_asa_eq_3,hr_asa_eq_3,rr_asa_eq_3 ) # normalize the test instance with the same approach of train ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train\n",
    "#X_test = heldout part...\n",
    "X_test_asa_eq_3 = preprocess(dbp_asa_eq_3,sbp_asa_eq_3,bt_asa_eq_3,hr_asa_eq_3,rr_asa_eq_3)\n",
    "X_test_asa_gt_3 = preprocess(dbp_asa_gt_3,sbp_asa_gt_3,bt_asa_gt_3,hr_asa_gt_3,rr_asa_gt_3 )\n",
    "\n",
    "res_train = model_first.predict(X_train)\n",
    "res_asa_eq_3 = model_first.predict(X_test_asa_eq_3)\n",
    "res_asa_gt_3 = model_first.predict(X_test_asa_gt_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train, dict_mae_train       = eval_error(X_train,res_train)\n",
    "mae_asa_eq_3, dict_mae_asa_eq_3 = eval_error(X_test_asa_eq_3,res_asa_eq_3)\n",
    "mae_asa_gt_3, dict_mae_asa_gt_3 = eval_error(X_test_asa_gt_3,res_asa_gt_3)\n",
    "\n",
    "print(\"MAE on train set:\")\n",
    "print(dict_mae_train )\n",
    "print(\"\\nMAE on asa equal to 3:\")\n",
    "print(dict_mae_asa_eq_3 )\n",
    "print(\"\\nMAE on asa greater than 3:\")\n",
    "print(dict_mae_asa_gt_3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = select_threshold(X_train, res_train, mae_train, perc=.98)\n",
    "idx_anomalies_asa_eq_3 = detect_anomalies(X_test_asa_eq_3, res_asa_eq_3, threshold)\n",
    "idx_anomalies_asa_gt_3 = detect_anomalies(X_test_asa_gt_3, res_asa_gt_3, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the presumed anomalies\n",
    "plot_predict(X_test_asa_eq_3,res_asa_eq_3,n=idx_anomalies_asa_eq_3[0],single=True,figsize = (50,50))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_thresholds = select_features_threshold(X_train, res_train, dict_mae_train, perc=.98)\n",
    "idx_features_anomalies_asa_eq_3  = detect_features_anomalies(X_test_asa_eq_3, res_asa_eq_3, feat_thresholds, out=True)\n",
    "idx_features_anomalies_asa_gt_3 = detect_features_anomalies(X_test_asa_gt_3, res_asa_gt_3, feat_thresholds, out=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26b7942ba83b19233e69a7a274947cfe2d06b8440a92717f153f734c87c5d304"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
