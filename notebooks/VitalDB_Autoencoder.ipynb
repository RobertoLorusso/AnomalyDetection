{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DO/BUGS/Considerations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUGS\n",
    "\n",
    "- When introducing the ECG as feature, the prediction gives nan values. Need to try to implement a single feature LSTM autoencoder on the ECG and see if the behaviour is different\n",
    "\n",
    "- Pad sequences with the mean of the feature may force the autoencoder to learn the mean instead the original ditribution. This problem occurs with HR,RR and DBP. TIME_STEP has to be setted to minimum lenght of features sequences.\n",
    "\n",
    "\n",
    "### TO-DO\n",
    "\n",
    "- Describe methodology: holdout method, preprocessing, multivariate lstm autoencoder, loss function (mse), reconstruction error, time for training, mean reconstructino error approach and adaptive thresholding based on it. Experimental results.\n",
    "\n",
    "- 'if item > 0' condition when flattening inside normalization functions should be in clean_data function\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "- Should I use also the std of reconstruction error for threshold estimation, as described in the work Adaptive Threshold for Outlier Detection on Data Streams?\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- Nel caricare i dati la maggior parte dei sample era costituito da valori NaN. Una volta rimossi si ottengono dei valori con dei picchi, dovuti all'aver reso contigui valori che prima non lo erano.\n",
    "- Rimossi i record che hanno una lunghezza pari a zero o una media negativa.\n",
    "- L'errore di ricostruzione è minore usando la normalizzazione minmax, probabilmente perchè i dati non hanno una distribuzione gaussiana\n",
    "- TIME_STEP maggiori degradano le prestazioni"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 08:52:27.146891: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import vitaldb\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import operator\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, TimeDistributed, RepeatVector\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import regularizers\n",
    "from seaborn import histplot\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_from_API(n_cases=None, asa = 3 , op = '<'):\n",
    "\n",
    "    \"\"\" \n",
    "    This function loads the VitalDB data from APIs\n",
    "\n",
    "    INPUT\n",
    "\n",
    "    - n_cases: limit the number of samples to load\n",
    "    - asa: ASA status for health status classification, can range in [1,2,3,4,5,6]\n",
    "    - op: operator to use on asa status. Permits to load a specific portion of the dataset.\n",
    "    \n",
    "     \n",
    "    OUTPUT  \n",
    "\n",
    "    - dbp: diastolyc blood pressure samples\n",
    "    - sbp: systolic blood pressure samples\n",
    "    - bt: body temperature samples\n",
    "    - hr: heart rate samples\n",
    "    - rr: respiratory rate samples\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ops = {'>': operator.gt,\n",
    "       '<': operator.lt,\n",
    "       '>=': operator.ge,\n",
    "       '<=': operator.le,\n",
    "       '==': operator.eq}\n",
    "\n",
    "    caseids_all = vitaldb.find_cases(['ART_DBP','ART_SBP','BT','HR','RR']) # find ids of patient with this parameters\n",
    "    \n",
    "    df = pd.read_csv('https://api.vitaldb.net/cases') # Load dataset\n",
    "    df = df[ops[op](df['asa'], asa)] # ASA param identifies the health status of the patient\n",
    "\n",
    "    caseids_unhealthy = df['caseid'].to_numpy() \n",
    "    caseids = [el for el in caseids_all if el in caseids_unhealthy]\n",
    "\n",
    "    if(n_cases is None):\n",
    "        n_cases = len(caseids)\n",
    "\n",
    "    dbp = []\n",
    "    sbp = []\n",
    "    bt  = []\n",
    "    hr  = []\n",
    "    rr  = []\n",
    "\n",
    "     # load all the patients data \n",
    "    for i in range(0,n_cases): # Select only five patient for testing purpose; then len(caseids)\n",
    "        try:\n",
    "            vals = vitaldb.load_case(caseids[i], ['ART_DBP','ART_SBP','BT','HR','RR'])\n",
    "            dbp.append(vals[:,0])\n",
    "            sbp.append(vals[:,1])\n",
    "            bt.append(vals[:,2])\n",
    "            hr.append(vals[:,3])\n",
    "            rr.append(vals[:,4])\n",
    "            # extract non-null values\n",
    "            dbp[i] = dbp[i][~np.isnan(dbp[i])]  \n",
    "            sbp[i] = sbp[i][~np.isnan(sbp[i])] \n",
    "            bt[i] = bt[i][~np.isnan(bt[i])]\n",
    "            hr[i] = hr[i][~np.isnan(hr[i])] \n",
    "            rr[i] = rr[i][~np.isnan(rr[i])]\n",
    "        except Exception as e: \n",
    "            print('\\n=================\\n')\n",
    "            print('INDEX: '+str(i))\n",
    "            print('ERROR: '+str(type(e)))\n",
    "            print('\\n=================\\n')\n",
    "            pass\n",
    "\n",
    "    \n",
    "    return dbp,sbp,bt,hr,rr\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disk data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_disk(path,name = 'numeric_data.vitaldb' ):\n",
    "    \n",
    "    dbp = []\n",
    "    sbp = []\n",
    "    bt  = []\n",
    "    hr  = []\n",
    "    rr  = []\n",
    "\n",
    "# save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "    filepath = os.path.join(path,name)\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        (dbp,sbp,bt,hr,rr) = pickle.load(f)\n",
    "    \n",
    "    return dbp,sbp,bt,hr,rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_disk(dbp,sbp,bt,hr,rr,path,name = 'numeric_data.vitaldb',):\n",
    "# save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "    with open(os.path.join(path,name), 'wb') as f:\n",
    "        pickle.dump((dbp,sbp,bt,hr,rr), f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(dbp,sbp,bt,rr,hr):\n",
    "\n",
    "    \"\"\" \n",
    "    INPUT \n",
    "\n",
    "    - dbp: diastolyc blood pressure samples\n",
    "    - sbp: systolic blood pressure samples\n",
    "    - bt: body temperature samples\n",
    "    - hr: heart rate samples\n",
    "    - rr: respiratory rate samples\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # remove all the negative values since these vital parameters can only be positive\n",
    "    flat_list_dbp = np.asarray([item for sublist in dbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_sbp = np.asarray([item for sublist in sbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_hr = np.asarray([item for sublist in hr for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_bt = np.asarray([item for sublist in bt  for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_rr = np.asarray([item for sublist in rr for item in sublist if item > 0],dtype='float64')\n",
    "\n",
    "    d = {}\n",
    "    d['Dyastolic BP'] = flat_list_dbp[:]\n",
    "    d['Systolic BP'] = flat_list_sbp[:]\n",
    "    d['Heart rate'] = flat_list_hr[:]\n",
    "\n",
    "    b = {}\n",
    "    b['Body temperature'] = flat_list_bt[:]\n",
    "    b['Respiratory rate'] = flat_list_rr[:]\n",
    "\n",
    "    f,ax = plt.subplots(2,1,figsize=(10,10))\n",
    "    histplot(d,binwidth=1,ax=ax[0])\n",
    "    histplot(b,binwidth=1,ax=ax[1])\n",
    "\n",
    "\n",
    "\n",
    "def plot_data(dbp,sbp,bt,hr,rr,bins=10):\n",
    "    \"\"\" \n",
    "    INPUT \n",
    "\n",
    "    - dbp: diastolyc blood pressure samples\n",
    "    - sbp: systolic blood pressure samples\n",
    "    - bt: body temperature samples\n",
    "    - hr: heart rate samples\n",
    "    - rr: respiratory rate samples\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    flat_list_dbp = np.asarray([item for sublist in dbp for item in sublist if item > 0 ],dtype='float64')\n",
    "    flat_list_sbp = np.asarray([item for sublist in sbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_bt = np.asarray([item for sublist in bt for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_hr = np.asarray([item for sublist in hr for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_rr = np.asarray([item for sublist in rr for item in sublist if item > 0],dtype='float64')\n",
    "    \n",
    "\n",
    "    plt.subplots_adjust(hspace=1.)\n",
    "    plt.subplot(511)\n",
    "    plt.title(\"DBP\")\n",
    "    plt.hist(flat_list_dbp,bins=bins)\n",
    "\n",
    "    plt.subplots_adjust(hspace=1.)\n",
    "    plt.subplot(512)\n",
    "    plt.title(\"SBP\")\n",
    "    plt.hist(flat_list_sbp,bins=bins)\n",
    "\n",
    "    plt.subplot(513)\n",
    "    plt.title(\"Body temperature\")\n",
    "    plt.hist(flat_list_bt,bins=bins)\n",
    "\n",
    "    plt.subplot(514)\n",
    "    plt.title(\"Heart rate\")\n",
    "    plt.hist(flat_list_hr,bins=bins)\n",
    "\n",
    "    plt.subplot(515)\n",
    "    plt.title(\"Respiratory rate\")\n",
    "    plt.hist(flat_list_rr,bins=bins)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(dbp,sbp,bt,hr,rr,raw = True):\n",
    "    \"\"\" \n",
    "    This function removes time series records which have either zero length (empty record) or negative mean value (affected by registration noise)\n",
    "\n",
    "    INPUT \n",
    "\n",
    "    - dbp: diastolyc blood pressure samples\n",
    "    - sbp: systolic blood pressure samples\n",
    "    - bt: body temperature samples\n",
    "    - hr: heart rate samples\n",
    "    - rr: respiratory rate samples\n",
    "\n",
    "    OUTPUT: cleaned data\n",
    "\n",
    "    NB: Blood pressure (dbp,sbp) ranges accordingly with the European guidelines ESC-ESH 2018.\n",
    "        Body temperature ranges in 35-37 \n",
    "        Hearth rate ranges in 60-100\n",
    "        Respiratory rate ranges in 10-21\n",
    "\n",
    "        These values for ranges are a bit wider than optimal ones since we want to preserve some variablity in the data. \n",
    "     \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(dbp)):\n",
    "            if(not raw):\n",
    "                dbp[i] = dbp[i][dbp[i] > 50]\n",
    "                dbp[i] = dbp[i][dbp[i] < 90]  # we want to preserve some variability\n",
    "\n",
    "        for i in range(0,len(dbp) - len([el for el in dbp if len(el) == 0 or (np.mean(el) <= 0)])):\n",
    "            if(len(dbp[i])==0 or (np.mean(dbp[i]) <= 0)):\n",
    "                dbp.pop(i)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(sbp)):\n",
    "            if(not raw):\n",
    "                sbp[i] = sbp[i][sbp[i] > 80]\n",
    "                sbp[i] = sbp[i][sbp[i] < 140] # we want to preserve some variability\n",
    "\n",
    "        for i in range(0,len(sbp) - len([el for el in sbp if len(el) == 0 or (np.mean(el) <= 0)])):\n",
    "            if(len(sbp[i])==0 or (np.mean(sbp[i]) <= 0)):\n",
    "                sbp.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(bt)):\n",
    "            if(not raw):\n",
    "                bt[i] = bt[i][bt[i] > 35]\n",
    "                bt[i] = bt[i][bt[i] < 37.2]\n",
    "        for i in range(0,len(bt) - len([el for el in bt if len(el) == 0 or (np.mean(el) <= 0)])):\n",
    "            if(len(bt[i])==0 or (np.mean(bt[i]) <= 0)):\n",
    "                bt.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(hr)):\n",
    "            if(not raw):\n",
    "                hr[i] = hr[i][hr[i] > 60]\n",
    "                hr[i] = hr[i][hr[i] < 100] # we want to preserve some variability\n",
    "\n",
    "        for i in range(0,len(hr) - len([el for el in hr if len(el) == 0 or (np.mean(el) <= 0)])):\n",
    "            if(len(hr[i])==0 or (np.mean(hr[i]) <= 0)):\n",
    "                hr.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for i in range(0,len(rr)):\n",
    "            if(not raw):\n",
    "                rr[i] = rr[i][rr[i] > 10]\n",
    "                rr[i] = rr[i][rr[i] < 21] # we want to preserve some variability\n",
    "\n",
    "        for i in range(0,len(rr) - len([el for el in rr if len(el) == 0 or (np.mean(el) <= 0)])):\n",
    "            if(len(rr[i])==0 or (np.mean(rr[i]) <= 0)):\n",
    "                rr.pop(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return dbp,sbp,bt,hr,rr\n",
    "\n",
    "\n",
    "def mean_norm(dbp,sbp,bt,hr,rr):\n",
    "\n",
    "    # remove all the negative values since these vital parameters can only be positive\n",
    "    flat_list_dbp = np.asarray([item for sublist in dbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_sbp = np.asarray([item for sublist in sbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_hr = np.asarray([item for sublist in hr for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_bt = np.asarray([item for sublist in bt  for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_rr = np.asarray([item for sublist in rr for item in sublist if item > 0],dtype='float64')\n",
    "    print({'dbp':len(dbp),'sbp':len(sbp),'hr':len(hr),'bt':len(bt),'rr':len(rr)})\n",
    "\n",
    "\n",
    "\n",
    "    dbp_mean,dbp_std = np.mean(flat_list_dbp),np.std(flat_list_dbp)\n",
    "    sbp_mean,sbp_std = np.mean(flat_list_sbp),np.std(flat_list_sbp)\n",
    "    bt_mean,bt_std = np.mean(flat_list_bt),np.std(flat_list_bt)\n",
    "    hr_mean,hr_std = np.mean(flat_list_hr),np.std(flat_list_hr)\n",
    "    rr_mean,rr_std = np.mean(flat_list_rr),np.std(flat_list_rr)\n",
    "\n",
    "    print('\\nMean values for features:')\n",
    "    print({'dbp':dbp_mean,'sbp':sbp_mean,'hr':hr_mean,'bt':bt_mean,'rr':rr_mean})\n",
    "    print('\\nStd values for features:')\n",
    "    print({'dbp':dbp_std,'sbp':sbp_std,'hr':hr_std,'bt':bt_std,'rr':rr_std})\n",
    "    print('\\n')\n",
    "\n",
    "    # Consider runtime warnings such as Divide by zero as Exceptions to throw\n",
    "    old_settings = np.seterr(divide='raise')\n",
    "    idx_remove = {}\n",
    "    idx_remove['sbp'] = []\n",
    "    idx_remove['dbp'] = []\n",
    "    idx_remove['bt'] = []\n",
    "    idx_remove['rr'] = []\n",
    "    idx_remove['hr'] = []\n",
    "    \n",
    "\n",
    "    for i in range(0,len(dbp)): \n",
    "        try:\n",
    "            dbp[i] = (dbp[i] - dbp_mean)/(dbp_std) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['dbp'].append(i)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(0,len(sbp)): \n",
    "        try:\n",
    "            sbp[i] = (sbp[i] - sbp_mean)/(sbp_std) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['sbp'].append(i)\n",
    "        \n",
    "\n",
    "\n",
    "    for i in range(0,len(bt)): \n",
    "        try:\n",
    "            bt[i] = (bt[i] - bt_mean)/(bt_std) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['bt'].append(i)\n",
    "\n",
    "\n",
    "    for i in range(0,len(hr)): \n",
    "        try:\n",
    "            hr[i] = (hr[i] - hr_mean)/(hr_std) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['hr'].append(i)\n",
    "\n",
    "    for i in range(0,len(rr)): \n",
    "        try:\n",
    "            rr[i] = (rr[i] - rr_mean)/(rr_std) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['hr'].append(i)\n",
    "\n",
    "    # Back to default settings for errors\n",
    "    np.seterr(**old_settings)\n",
    "\n",
    "    return dbp,sbp,bt,hr,rr,idx_remove\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def minmax_norm(dbp,sbp,bt,hr,rr):\n",
    "\n",
    "\n",
    "    # remove all the negative values since these vital parameters can only be positive\n",
    "    flat_list_dbp = np.asarray([item for sublist in dbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_sbp = np.asarray([item for sublist in sbp for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_hr = np.asarray([item for sublist in hr for item in sublist if item > 0],dtype='float64')\n",
    "    flat_list_bt = np.asarray([item for sublist in bt  for item in sublist if item > 0] ,dtype='float64')\n",
    "    flat_list_rr = np.asarray([item for sublist in rr for item in sublist if item > 0],dtype='float64')\n",
    "    #print({'dbp':len(flat_list_dbp),'sbp':len(flat_list_sbp),'hr':len(flat_list_hr),'bt':len(flat_list_bt),'rr':len(flat_list_rr)})\n",
    "\n",
    "\n",
    "    dbp_min,dbp_max = np.min(flat_list_dbp),np.max(flat_list_dbp)\n",
    "    sbp_min,sbp_max = np.min(flat_list_sbp),np.max(flat_list_sbp)\n",
    "    bt_min,bt_max = np.min(flat_list_bt),np.max(flat_list_bt)\n",
    "    hr_min,hr_max = np.min(flat_list_hr),np.max(flat_list_hr)\n",
    "    rr_min,rr_max = np.min(flat_list_rr),np.max(flat_list_rr)\n",
    "    \n",
    "    old_settings = np.seterr(divide='raise')\n",
    "    idx_remove = {}\n",
    "\n",
    "    idx_remove['sbp'] = []\n",
    "    idx_remove['dbp'] = []\n",
    "    idx_remove['bt'] = []\n",
    "    idx_remove['rr'] = []\n",
    "    idx_remove['hr'] = []\n",
    "\n",
    "    for i in range(0,len(dbp)): \n",
    "        try:\n",
    "            dbp[i] = (dbp[i] - dbp_min)/(dbp_max - dbp_min) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['dbp'].append(i)\n",
    "\n",
    "\n",
    "    for i in range(0,len(sbp)): \n",
    "        try:\n",
    "            sbp[i] = (sbp[i] - sbp_min)/(sbp_max - sbp_min) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['sbp'].append(i)\n",
    "\n",
    "\n",
    "    for i in range(0,len(bt)): \n",
    "        try:\n",
    "            bt[i] = (bt[i] - bt_min)/(bt_max - bt_min) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['bt'].append(i)\n",
    "\n",
    "\n",
    "    for i in range(0,len(hr)): \n",
    "        try:\n",
    "            hr[i] = (hr[i] - hr_min)/(hr_max - hr_min) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['hr'].append(i)\n",
    "\n",
    "\n",
    "    for i in range(0,len(rr)): \n",
    "        try:\n",
    "            rr[i] = (rr[i] - rr_min)/(rr_max - rr_min) \n",
    "        except: \n",
    "            # remove values for which the normalization gives Runtime warning\n",
    "            idx_remove['rr'].append(i)\n",
    "\n",
    "\n",
    "    # Back to default settings for errors\n",
    "    np.seterr(**old_settings)\n",
    "\n",
    "    return dbp,sbp,bt,hr,rr,idx_remove\n",
    "\n",
    "\n",
    "\n",
    "def get_preprocessed_data(path, raw = False, norm_func = None):\n",
    "\n",
    "    dbp,sbp,bt,hr,rr = load_from_disk(path=path)\n",
    "    dbp,sbp,bt,hr,rr = clean_data(dbp,sbp,bt,hr,rr,raw)\n",
    "\n",
    "    idx_remove = []\n",
    "\n",
    "    if(norm_func is not None):\n",
    "        dbp,sbp,bt,hr,rr,idx_remove = norm_func(dbp,sbp,bt,hr,rr)\n",
    "        \n",
    "    return np.asarray(dbp),np.asarray(sbp),np.asarray(bt),np.asarray(hr),np.asarray(rr),idx_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9q/vg5rvmpn2hv8_mc8vpz6y9x80000gp/T/ipykernel_12343/2849605582.py:255: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(dbp),np.asarray(sbp),np.asarray(bt),np.asarray(hr),np.asarray(rr),idx_remove\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/Roberto/projects/AnomalyDetection/data/processed'\n",
    "norm = 'minmax'\n",
    "try:\n",
    "    func_norm = norm + '_norm'\n",
    "    normalize = globals()[func_norm] # normalize is called later for test data normalization\n",
    "except:\n",
    "    normalize = None\n",
    "\n",
    "dbp,sbp,bt,hr,rr,idx_remove = get_preprocessed_data(path = path,norm_func = normalize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declarations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of sequences between features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4658.74582172702"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TIME_STEP of preprocess function should be equal to the minimum length of features sequences\n",
    "min_seq_len = np.min([np.mean([len(el) for el in dbp]),np.mean([len(el) for el in sbp]),np.mean([len(el) for el in bt]),np.mean([len(el) for el in rr]),np.mean([len(el) for el in hr])])\n",
    "print('Mean length of sequences between features')\n",
    "min_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dbp,sbp,bt,hr,rr, TIME_STEP = 5500, n_instances = None):\n",
    "\n",
    "    \"\"\"\n",
    "    This function prepare the data to be input to the LSTM autoencoder, both for training or testing purpose.\n",
    "    This function assumes that the input data is already normalized, thus NO values modifications are needed when flattening.\n",
    "\n",
    "    INPUT \n",
    "\n",
    "    - dbp: diastolyc blood pressure samples\n",
    "    - sbp: systolic blood pressure samples\n",
    "    - bt: body temperature samples\n",
    "    - hr: heart rate samples\n",
    "    - rr: respiratory rate samples\n",
    "\n",
    "    - TIME_STEP: length of sequences for lstm autoencoder. Default to 5500 since it's the mean length of features sequences\n",
    "    - n_instances: number of instances to retain. Useful for testing purposes i.e. training the data on a smaller portion of data to computational resources and testing time.\n",
    "\n",
    "    OUTPUT\n",
    "\n",
    "    - X: tensor to be input to the LSTM autoencoder\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # flat the data to be later chuncked in sequences\n",
    "\n",
    "    flat_list_dbp = np.asarray([item for sublist in dbp for item in sublist],dtype='float64')\n",
    "    flat_list_sbp = np.asarray([item for sublist in sbp for item in sublist],dtype='float64')\n",
    "    flat_list_bt = np.asarray([item for sublist in bt for item in sublist],dtype='float64')\n",
    "    flat_list_hr = np.asarray([item for sublist in hr for item in sublist],dtype='float64')\n",
    "    flat_list_rr = np.asarray([item for sublist in rr for item in sublist],dtype='float64')\n",
    "\n",
    "\n",
    "\n",
    "    X_dbp = []\n",
    "    X_sbp = []\n",
    "    X_bt = []\n",
    "    X_hr = []\n",
    "    X_rr = []\n",
    "\n",
    "    for seq in range(0,len(flat_list_dbp), TIME_STEP):\n",
    "        X_dbp.append(flat_list_dbp[seq:seq+TIME_STEP])\n",
    "\n",
    "    for seq in range(0,len(flat_list_sbp), TIME_STEP):\n",
    "        X_sbp.append(flat_list_sbp[seq:seq+TIME_STEP])\n",
    "\n",
    "    for seq in range(0,len(flat_list_bt), TIME_STEP):\n",
    "        X_bt.append(flat_list_bt[seq:seq+TIME_STEP])\n",
    "\n",
    "    for seq in range(0,len(flat_list_hr), TIME_STEP):\n",
    "        X_hr.append(flat_list_hr[seq:seq+TIME_STEP])\n",
    "\n",
    "    for seq in range(0,len(flat_list_rr), TIME_STEP):\n",
    "        X_rr.append(flat_list_rr[seq:seq+TIME_STEP])\n",
    "\n",
    "\n",
    "    X_dbp = np.asarray(X_dbp,dtype=object)\n",
    "    X_sbp = np.asarray(X_sbp,dtype=object)\n",
    "    X_bt = np.asarray(X_bt,dtype=object)\n",
    "    X_hr = np.asarray(X_hr,dtype=object)\n",
    "    X_rr = np.asarray(X_rr,dtype=object)\n",
    "\n",
    "    # Pad with mean value of data\n",
    "    X_dbp = pad_sequences(X_dbp, TIME_STEP,padding='post',value = np.mean(flat_list_dbp),dtype='float64')\n",
    "    X_sbp = pad_sequences(X_sbp, TIME_STEP,padding='post',value = np.mean(flat_list_sbp),dtype='float64')\n",
    "    X_bt = pad_sequences(X_bt, TIME_STEP,padding='post',value = np.mean(flat_list_bt),dtype='float64')\n",
    "    X_hr = pad_sequences(X_hr, TIME_STEP,padding='post',value = np.mean(flat_list_hr),dtype='float64')\n",
    "    X_rr = pad_sequences(X_rr, TIME_STEP,padding='post',value = np.mean(flat_list_rr),dtype='float64')\n",
    "\n",
    "\n",
    "\n",
    "    X_dbp = np.asarray(np.expand_dims(X_dbp,axis=2))\n",
    "    X_sbp = np.asarray(np.expand_dims(X_sbp,axis=2))\n",
    "    X_bt = np.asarray(np.expand_dims(X_bt,axis=2))\n",
    "    X_hr = np.asarray(np.expand_dims(X_hr,axis=2))\n",
    "    X_rr = np.asarray(np.expand_dims(X_rr,axis=2))\n",
    "\n",
    "    # Truncate on the minimum length of features sequences\n",
    "    min_len = np.min([X_dbp.shape[0],X_sbp.shape[0],X_bt.shape[0],X_hr.shape[0],X_rr.shape[0]])\n",
    "    \n",
    "    X_dbp = X_dbp[:min_len,:]\n",
    "    X_sbp = X_sbp[:min_len,:]\n",
    "    X_bt = X_bt[:min_len,:]\n",
    "    X_hr = X_hr[:min_len,:]\n",
    "    X_rr = X_rr[:min_len,:]\n",
    "\n",
    "    Y = np.concatenate([X_dbp, X_sbp, X_bt, X_hr, X_rr],axis=2)\n",
    "    \n",
    "    if(isinstance(n_instances,int) and n_instances > 0):\n",
    "        X = Y[:n_instances] # Select the first n instances\n",
    "    else:\n",
    "        X = Y\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def eval_error(X_test,res,std=False):\n",
    "\n",
    "    \"\"\"\n",
    "    This function evaluates the reconstruction error of the input samples, \n",
    "    returning the mean reconstruction error for every feature and for the overall samples.\n",
    "\n",
    "    INPUT\n",
    "\n",
    "    - X_test : test samples\n",
    "    - res_test: reconstructed test samples\n",
    "    - std: Boolean, return standard devation for reconstruction error of the features\n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    - overall_mean: mean reconstruction error for the entire input data\n",
    "    - dict_means: Dictionary, contains the mean reconstruction error for the single features\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n = min(X_test.shape[0],res.shape[0])\n",
    "    n_features = min(X_test.shape[2],res.shape[2])\n",
    "\n",
    "    overall_mean = 0\n",
    "\n",
    "    dict_means = {}\n",
    "    dict_means['sbp'] = 0\n",
    "    dict_means['dbp'] = 0\n",
    "    dict_means['bt'] = 0\n",
    "    dict_means['rr'] = 0\n",
    "    dict_means['hr'] = 0\n",
    "\n",
    "    dic_means = {}\n",
    "    dic_means[0] = 0\n",
    "    dic_means[1] = 0\n",
    "    dic_means[2] = 0\n",
    "    dic_means[3] = 0\n",
    "    dic_means[4] = 0\n",
    "\n",
    "    if(std):\n",
    "        dict_std = {}\n",
    "        dict_std['sbp'] = 0\n",
    "        dict_std['dbp'] = 0\n",
    "        dict_std['bt'] = 0\n",
    "        dict_std['rr'] = 0\n",
    "        dict_std['hr'] = 0\n",
    "\n",
    "        dic_std = {}\n",
    "        dic_std[0] = 0\n",
    "        dic_std[1] = 0\n",
    "        dic_std[2] = 0\n",
    "        dic_std[3] = 0\n",
    "        dic_std[4] = 0\n",
    "    \n",
    "\n",
    "    for i in range(0,n_features):\n",
    "\n",
    "        tot = 0\n",
    "        for p in range(0,n):\n",
    "            re = np.linalg.norm(X_test[p,:,i] - res[p,:,i],2)\n",
    "            tot = tot + re            \n",
    "            dic_means[i] = dic_means[i] + re\n",
    " \n",
    "        mean = np.round(tot/n,3)\n",
    "        overall_mean = overall_mean + mean\n",
    "        \n",
    "        if(std):\n",
    "            # compute the variance\n",
    "            for p in range(0,n):\n",
    "                re = np.linalg.norm(X_test[p,:,i] - res[p,:,i],2)\n",
    "                dic_std[i] = (re - mean)**2 + dic_std[i]\n",
    "\n",
    "\n",
    "\n",
    "    dict_means['sbp'] = np.round(dic_means[0]/n,4)\n",
    "    dict_means['dbp'] = np.round(dic_means[1]/n,4)\n",
    "    dict_means['bt']  = np.round(dic_means[2]/n,4)\n",
    "    dict_means['rr']  = np.round(dic_means[3]/n,4)\n",
    "    dict_means['hr']  = np.round(dic_means[4]/n,4)\n",
    "\n",
    "    if(std):\n",
    "        dict_std['sbp'] = np.round((dic_std[0]/n)**(1/2),4)\n",
    "        dict_std['dbp'] = np.round((dic_std[1]/n)**(1/2),4)\n",
    "        dict_std['bt']  = np.round((dic_std[2]/n)**(1/2),4)\n",
    "        dict_std['rr']  = np.round((dic_std[3]/n)**(1/2),4)\n",
    "        dict_std['hr']  = np.round((dic_std[4]/n)**(1/2),4)\n",
    "        del dic_std\n",
    "\n",
    "    del dic_means\n",
    "    \n",
    "\n",
    "    overall_mean = overall_mean/n_features\n",
    "\n",
    "    if(std):\n",
    "        return overall_mean,dict_means,dict_std\n",
    "    else:\n",
    "        return overall_mean,dict_means\n",
    "\n",
    "\n",
    "\n",
    "def select_threshold(X, res, mean_rec_err, perc=.98, nit=10000, step=None, out=False):\n",
    "\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "\n",
    "    - X : training data of the autoencoder\n",
    "    - res: reconstructed data from autoencoder\n",
    "    - mean_rec_err:  mean reconstruction error obtained on training data\n",
    "    - perc: percentage of training data for which the reconstruction error must be below the mean_rec_err multiplied by a factor\n",
    "    - nit: max number of iterations\n",
    "    - step: step with which increase the multiplication factor at every step\n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    - threshold: threshold for reconstruction error on test data, for which considering a sample anomalous\n",
    "\n",
    "    NB: An higher percentage value will increase the threshold, resulting in a looser anomaly detection. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    counter = 0\n",
    "    target = 0\n",
    "    dim = X.shape[0]\n",
    "    if(step is None):\n",
    "        step = 1 - perc # step estimation from perc for a better accuracy in factor estimation\n",
    "    factor = 1 - step # This subtraction is needed for the factor to be equal to one at the first step of while cycle\n",
    "    \n",
    "    while( target < perc and counter < nit):\n",
    "        j = 0\n",
    "        counter = counter + 1\n",
    "        factor = factor + step\n",
    "\n",
    "        for n in range(dim):\n",
    "            err = np.linalg.norm(X[n,...]-res[n,...],2)\n",
    "            if(abs(err <= mean_rec_err * factor)):\n",
    "                j = j+1\n",
    "        target = np.round(j/dim,2)\n",
    "        if(out):\n",
    "            print(target,factor)\n",
    "    \n",
    "    threshold = np.round(factor * mean_rec_err,3)\n",
    "    return threshold\n",
    "\n",
    "\n",
    "\n",
    "def select_features_threshold(X, res, dict_means, perc=.98, nit=10000, step=None, out=False):\n",
    "\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "\n",
    "    - X : training data of the autoencoder\n",
    "    - res: reconstructed data from autoencoder\n",
    "    - mean_rec_err:  mean reconstruction error obtained on training data\n",
    "    - perc: percentage of training data for which the reconstruction error must be below the mean_rec_err multiplied by a factor\n",
    "    - nit: max number of iterations\n",
    "    - step: step with which increase the multiplication factor at every step\n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    - thresholds: threshold for reconstruction error on test data, for which considering a sample anomalous\n",
    "\n",
    "    Note: An anomaly will be a sample that differs from the percentage of data specified.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    dim = X.shape[0]\n",
    "    n_features = X.shape[2]\n",
    "    means_rec_err = [dict_means[k] for k in dict_means.keys()]\n",
    "    factors = np.zeros(len(means_rec_err))\n",
    "    thresholds = np.zeros(len(means_rec_err))\n",
    "\n",
    "    if(step is None):\n",
    "        step = 1 - perc # step estimation from perc for a better accuracy in factor estimation\n",
    "    factors[:] = 1 - step\n",
    "    \n",
    "    for f in range(0,n_features):\n",
    "\n",
    "        counter = 0 # iterations counter\n",
    "        target = 0\n",
    "        if(out):\n",
    "            print('\\n\\t\\t\\tFEATURE ' + str(f))\n",
    "        while( target < perc and counter < nit):\n",
    "\n",
    "            j = 0 # numerator of percentage estimation j/dim\n",
    "            counter = counter + 1\n",
    "            factors[f] = factors[f] + step\n",
    "\n",
    "            for n in range(dim):\n",
    "                err = np.linalg.norm(X[n,:,f]-res[n,:,f],2)\n",
    "                if(abs(err <= means_rec_err[f]*factors[f])):\n",
    "                    j = j+1\n",
    "\n",
    "            target = np.round(j/dim,2)\n",
    "            if(out):\n",
    "                print(target,factors[f])\n",
    "\n",
    "        thresholds[f] = np.round(factors[f]*means_rec_err[f],3)\n",
    "\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "\n",
    "def detect_anomalies(X_test,res_test,threshold,out=True):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "\n",
    "    - X_test : test samples\n",
    "    - res_test: reconstructed test samples\n",
    "    - threshold: threshold for reconstruction error on test data, for which considering a sample anomalous. See select_threshold for details\n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    - idx_anomalies: list of indices of test samples deemed anomalous on the basis of the given threshold.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dim = X_test.shape[0]\n",
    "    idx_anomalies = []\n",
    "\n",
    "    for i in range(dim):\n",
    "        err = np.linalg.norm(X_test[i,...]-res_test[i,...],2) \n",
    "        if(abs(err > threshold)):\n",
    "            idx_anomalies.append(i)\n",
    "    \n",
    "    if(out):\n",
    "        print('\\nPercentage of anomalies in tested data:' + str(np.round(len(idx_anomalies)/X_test.shape[0],2)))\n",
    "\n",
    "    return idx_anomalies\n",
    "\n",
    "\n",
    "\n",
    "def detect_features_anomalies(X_test,res_test,thresholds,out=False):\n",
    "\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "\n",
    "    - X_test : test samples\n",
    "    - res_test: reconstructed test samples\n",
    "    - threshold: threshold for reconstruction error on test data, for which considering a sample anomalous. See select_threshold for details\n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    - idx_anomalies: list of indices of test samples deemed anomalous on the basis of the given threshold.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dim,n_features = X_test.shape[0],X_test.shape[2]\n",
    "    idx_anomalies = {}\n",
    "    idx_anomalies['sbp'] = []\n",
    "    idx_anomalies['dbp'] = []\n",
    "    idx_anomalies['bt'] = []\n",
    "    idx_anomalies['rr'] = []\n",
    "    idx_anomalies['hr'] = []\n",
    "\n",
    "    dic = {}\n",
    "    dic[0] = []\n",
    "    dic[1] = []\n",
    "    dic[2] = []\n",
    "    dic[3] = []\n",
    "    dic[4] = []\n",
    "\n",
    "    for f in range(0,n_features):\n",
    "        for i in range(dim):\n",
    "            err = np.linalg.norm(X_test[i,:,f] - res_test[i,:,f],2) \n",
    "            if(abs(err > thresholds[f])):\n",
    "                dic[f].append(i)\n",
    "\n",
    "    idx_anomalies['sbp'] = dic[0]\n",
    "    idx_anomalies['dbp'] = dic[1]\n",
    "    idx_anomalies['bt'] = dic[2]\n",
    "    idx_anomalies['rr'] = dic[3]\n",
    "    idx_anomalies['hr'] = dic[4]\n",
    "\n",
    "    if(out):\n",
    "        print('\\nPercentage of anomalies in tested data:')\n",
    "        for i in range(0,n_features):\n",
    "            print('\\nFEATURE ' + str(i) + ': ' + str(np.round(len(dic[i])/X_test.shape[0],2)))\n",
    "\n",
    "    del dic\n",
    "    return idx_anomalies\n",
    "\n",
    "\n",
    "def plot_predict(X_test, res, figsize=(40,40), n = None, single = False, vals = None):\n",
    "\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "\n",
    "    - X_test : data samples to plot\n",
    "    - res_test: reconstructed data samples to plot\n",
    "    - figsize: size of the plot\n",
    "    - n: number of samples to plot\n",
    "    - single: Boolean, when True plots the single instance number specified by 'n'\n",
    "    - vals: List of indices of samples to plot\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    if(n is not None and isinstance(n,int) and n > -1):\n",
    "        if(single):\n",
    "            for j in range(0,5):\n",
    "                s = '61'+str(j+1)\n",
    "                plt.subplot(int(s))\n",
    "                plt.subplots_adjust(hspace=1.)\n",
    "                plt.title('Feature ' + str(j+1))\n",
    "                plt.plot(res[n,:,j],'-r',linewidth=2.5,)\n",
    "                plt.plot(X_test[n,:,j],'-g',linewidth=2.5,alpha=0.8)\n",
    "            plt.show()\n",
    "        else:\n",
    "            for j in range(0,5):\n",
    "                for i in range(0,n):\n",
    "                    s = '61'+str(j+1)\n",
    "                    plt.subplot(int(s))\n",
    "                    plt.subplots_adjust(hspace=1.)\n",
    "                    plt.title('Feature ' + str(j+1))\n",
    "                    plt.plot(res[i,:,j],'-r',linewidth=2.5,)\n",
    "                    plt.plot(X_test[i,:,j],'-g',linewidth=2.,alpha=0.5)\n",
    "            plt.show()\n",
    "    elif(vals is not None):\n",
    "        for j in range(0,5):\n",
    "            for i in vals:\n",
    "                s = '61'+str(j+1)\n",
    "                plt.subplot(int(s))\n",
    "                plt.subplots_adjust(hspace=1.)\n",
    "                plt.title('Feature ' + str(j+1))\n",
    "                plt.plot(res[i,:,j],'-r',linewidth=2.5,)\n",
    "                plt.plot(X_test[i,:,j],'-g',linewidth=2.,alpha=0.5)\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        n = X_test.shape[0]\n",
    "        for j in range(0,5):\n",
    "            for i in range(0,n):\n",
    "                s = '61'+str(j+1)\n",
    "                plt.subplot(int(s))\n",
    "                plt.subplots_adjust(hspace=1.)\n",
    "                plt.title('Feature ' + str(j+1))\n",
    "                plt.plot(res[i,:,j],'-r',linewidth=2.5,)\n",
    "                plt.plot(X_test[i,:,j],'-g',linewidth=2.,alpha=0.5)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '/Users/Roberto/projects/AnomalyDetection/data/raw'\n",
    "path_model = '/Users/Roberto/projects/AnomalyDetection/models'\n",
    "\n",
    "X_train = preprocess(dbp,sbp,bt,hr,rr)\n",
    "# Holdout split\n",
    "n_train = int(X_train.shape[0]*0.7)\n",
    "X_test = X_train[n_train:,...]\n",
    "X_train = X_train[:n_train,...]\n",
    "\n",
    "\n",
    "batch_size, seq_len, n_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 5500, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 08:53:04.631270: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 5500, 256)         268288    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 5500, 256)         0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 5500, 128)         197120    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 5500, 64)          49408     \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 5500, 64)          0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 5500, 128)         98816     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 5500, 256)         394240    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 5500, 256)         0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 5500, 5)          1285      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,009,157\n",
      "Trainable params: 1,009,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, bias_regularizer=regularizers.l2(0.7),  recurrent_regularizer=regularizers.l2(0.6) , input_shape=(seq_len, n_features),return_sequences=True))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(LSTM(128,    bias_regularizer=regularizers.l2(0.7),  recurrent_regularizer=regularizers.l2(0.6),   return_sequences=True))\n",
    "model.add(LSTM(64,  bias_regularizer=regularizers.l1(0.7), recurrent_regularizer=regularizers.l2(0.6),  return_sequences=True))\n",
    "model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "model.add(LSTM(128,    bias_regularizer=regularizers.l2(0.7),  recurrent_regularizer=regularizers.l2(0.6),   return_sequences=True))\n",
    "model.add(LSTM(256,   bias_regularizer=regularizers.l2(0.7), recurrent_regularizer=regularizers.l2(0.6),  return_sequences=True))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, save and load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_first = tf.keras.models.load_model(os.path.join(path_model,\"first/autoencoder_LSTM_minmax_v3\"))\n",
    "#hist = model.fit(X_train,X_train, batch_size = 64, validation_split=0.1, verbose=1)\n",
    "#model.save(\"/Users/Roberto/projects/AnomalyDetection/models/first/autoencoder_LSTM_v3.h5\")\n",
    "#model.save(\"/Users/Roberto/projects/AnomalyDetection/models/first/autoencoder_LSTM_v3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_second = tf.keras.models.load_model(os.path.join(path_model,\"second/autoencoder_LSTM_v1\"))\n",
    "#model_second.fit(X_train,X_train, batch_size = 64, validation_split=0.1, verbose=1)\n",
    "#model_second.save(\"/Users/Roberto/projects/AnomalyDetection/models/second/autoencoder_LSTM_v1.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: if data il loaded from data/raw or from API need to call clean_data() and normalize()\n",
    "\n",
    "dbp_asa_eq_3,sbp_asa_eq_3,bt_asa_eq_3,hr_asa_eq_3,rr_asa_eq_3  = load_from_disk(path_data,'asa_eq_3.vitaldb')\n",
    "dbp_asa_gt_3,sbp_asa_gt_3,bt_asa_gt_3,hr_asa_gt_3,rr_asa_gt_3  = load_from_disk(path_data,'asa_gt_3.vitaldb')\n",
    "\n",
    "dbp_asa_eq_3,sbp_asa_eq_3,bt_asa_eq_3,hr_asa_eq_3,rr_asa_eq_3   = clean_data(dbp_asa_eq_3,sbp_asa_eq_3,bt_asa_eq_3,hr_asa_eq_3,rr_asa_eq_3,raw=True)\n",
    "dbp_asa_eq_3,sbp_asa_eq_3,bt_asa_eq_3,hr_asa_eq_3,rr_asa_eq_3,_ =  normalize(dbp_asa_eq_3,sbp_asa_eq_3,bt_asa_eq_3,hr_asa_eq_3,rr_asa_eq_3 ) # normalize the test instance with the same approach of train ones\n",
    "\n",
    "dbp_asa_gt_3,sbp_asa_gt_3,bt_asa_gt_3,hr_asa_gt_3,rr_asa_gt_3   = clean_data(dbp_asa_gt_3,sbp_asa_gt_3,bt_asa_gt_3,hr_asa_gt_3,rr_asa_gt_3,raw=True)\n",
    "dbp_asa_gt_3,sbp_asa_gt_3,bt_asa_gt_3,hr_asa_gt_3,rr_asa_gt_3,_ =  normalize(dbp_asa_gt_3,sbp_asa_gt_3,bt_asa_gt_3,hr_asa_gt_3,rr_asa_gt_3 ) # normalize the test instance with the same approach of train ones\n",
    "\n",
    "X_train = X_train\n",
    "X_test = X_test\n",
    "X_test_asa_eq_3 = preprocess(dbp_asa_eq_3,sbp_asa_eq_3,bt_asa_eq_3,hr_asa_eq_3,rr_asa_eq_3)\n",
    "X_test_asa_gt_3 = preprocess(dbp_asa_gt_3,sbp_asa_gt_3,bt_asa_gt_3,hr_asa_gt_3,rr_asa_gt_3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/63 [>.............................] - ETA: 12:33"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res_train \u001b[39m=\u001b[39m model_first\u001b[39m.\u001b[39;49mpredict(X_train)\n\u001b[1;32m      2\u001b[0m res_test \u001b[39m=\u001b[39m model_first\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      3\u001b[0m res_asa_eq_3 \u001b[39m=\u001b[39m model_first\u001b[39m.\u001b[39mpredict(X_test_asa_eq_3)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/keras/engine/training.py:2350\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2348\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[1;32m   2349\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 2350\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[1;32m   2351\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   2352\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    917\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 919\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    920\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    921\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res_train = model_first.predict(X_train)\n",
    "res_test = model_first.predict(X_test)\n",
    "res_asa_eq_3 = model_first.predict(X_test_asa_eq_3)\n",
    "res_asa_gt_3 = model_first.predict(X_test_asa_gt_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train, dict_mae_train       = eval_error(X_train,res_train)\n",
    "mae_asa_eq_3, dict_mae_asa_eq_3 = eval_error(X_test_asa_eq_3,res_asa_eq_3)\n",
    "mae_asa_gt_3, dict_mae_asa_gt_3 = eval_error(X_test_asa_gt_3,res_asa_gt_3)\n",
    "\n",
    "print(\"MAE on train set:\")\n",
    "print(dict_mae_train )\n",
    "print(\"MAE on test set:\")\n",
    "print(dict_mae_test )\n",
    "print(\"\\nMAE on asa equal to 3:\")\n",
    "print(dict_mae_asa_eq_3 )\n",
    "print(\"\\nMAE on asa greater than 3:\")\n",
    "print(dict_mae_asa_gt_3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = select_threshold(X_train, res_train, mae_train, perc=.98)\n",
    "idx_anomalies_test = detect_anomalies(X_test, res_test, threshold)\n",
    "idx_anomalies_asa_eq_3 = detect_anomalies(X_test_asa_eq_3, res_asa_eq_3, threshold)\n",
    "idx_anomalies_asa_gt_3 = detect_anomalies(X_test_asa_gt_3, res_asa_gt_3, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the presumed anomalies\n",
    "plot_predict(X_test_asa_eq_3,res_asa_eq_3,n=idx_anomalies_asa_eq_3[0],single=True,figsize = (50,50))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_thresholds = select_features_threshold(X_train, res_train, dict_mae_train, perc=.98)\n",
    "idx_features_anomalies_test  = detect_features_anomalies(X_test, res_test, feat_thresholds, out=True)\n",
    "idx_features_anomalies_asa_eq_3  = detect_features_anomalies(X_test_asa_eq_3, res_asa_eq_3, feat_thresholds, out=True)\n",
    "idx_features_anomalies_asa_gt_3 = detect_features_anomalies(X_test_asa_gt_3, res_asa_gt_3, feat_thresholds, out=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26b7942ba83b19233e69a7a274947cfe2d06b8440a92717f153f734c87c5d304"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
