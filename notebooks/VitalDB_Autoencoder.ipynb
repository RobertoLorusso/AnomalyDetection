{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vitaldb\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API data load and save on disk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read healthy patients data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseids_all = vitaldb.find_cases(['ECG_II','ART_DBP','ART_SBP','BT','HR','RR']) # find ids of patient with this parameters\n",
    "# Load dataset\n",
    "df = pd.read_csv('https://api.vitaldb.net/cases')\n",
    "df = df[df['asa'] < 3] # Extract ids of patients with asa < 3 and not dead in hospital\n",
    "df = df[df['death_inhosp'] == False]\n",
    "\n",
    "caseids_healthy = df['caseid'].to_numpy() # extract the case ids of patience with good health\n",
    "caseids = [el for el in caseids_all if el in caseids_healthy]\n",
    "print(len(caseids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = []\n",
    "dbp = []\n",
    "sbp = []\n",
    "bt  = []\n",
    "hr  = []\n",
    "rr  = []\n",
    "# load all the patients data \n",
    "for i in range(0,len(caseids)): # Select only five patient for testing purpose; then len(caseids)\n",
    "    try:\n",
    "        vals = vitaldb.load_case(caseids[i], ['ART_DBP','ART_SBP','BT','HR','RR'])\n",
    "        dbp.append(vals[:,0])\n",
    "        sbp.append(vals[:,1])\n",
    "        bt.append(vals[:,2])\n",
    "        hr.append(vals[:,3])\n",
    "        rr.append(vals[:,4])\n",
    "\n",
    "        # extract non-null values\n",
    "        dbp[i] = dbp[i][~np.isnan(dbp[i])]  \n",
    "        sbp[i] = sbp[i][~np.isnan(sbp[i])] \n",
    "        bt[i] = bt[i][~np.isnan(bt[i])]\n",
    "        hr[i] = hr[i][~np.isnan(hr[i])] \n",
    "        rr[i] = rr[i][~np.isnan(rr[i])]\n",
    "    \n",
    "    except Exception as e: \n",
    "        print('\\n=================\\n')\n",
    "        print('INDEX: '+str(i))\n",
    "        print('ERROR: '+str(type(e)))\n",
    "        print('\\n=================\\n')\n",
    "        pass\n",
    "\n",
    "\n",
    "for i in range(0,len(caseids)):\n",
    "    #vals = vitaldb.load_case(caseids[i], ['ECG_II'], 0.01) #parameter 0.01 for a 'zoomed' ecg\n",
    "    vals = vitaldb.load_case(caseids[i], ['ECG_II'])\n",
    "    ecg.append(vals[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "filepath = '/Users/Roberto/projects/siiaproject-vitanomaly/data.vitaldb'\n",
    "ecgpath = '/Volumes/Windows/SIIA/ecg_data.vitaldb'\n",
    "ecgpath = '/Users/Roberto/projects/siiaproject-vitanomaly/ecg_data.vitaldb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "\n",
    "with open(filepath, 'wb') as f:\n",
    "    pickle.dump((dbp,sbp,bt,hr,rr), f)\n",
    "\n",
    "with open(ecgpath, 'wb') as f:\n",
    "    pickle.dump(ecg, f)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = []\n",
    "dbp = []\n",
    "sbp = []\n",
    "bt  = []\n",
    "hr  = []\n",
    "rr  = []\n",
    "\n",
    "# save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "filepath = '/Users/Roberto/projects/AnomalyDetection/data/raw/numeric_data.vitaldb'\n",
    "ecgpath = '/Volumes/Windows/SIIA/ecg_data.vitaldb'\n",
    "ecgpath = '/Users/Roberto/projects/AnomalyDetection/data/raw/ecg_data.vitaldb'\n",
    "\n",
    "with open(ecgpath, 'rb') as f:\n",
    "    (ecg) = pickle.load(f)\n",
    "with open(filepath, 'rb') as f:\n",
    "    (dbp,sbp,bt,hr,rr) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty elements\n",
    "\n",
    "for i in range(0,len(dbp) - len([el for el in dbp if len(el) == 0])):\n",
    "    if(len(dbp[i])==0):\n",
    "        dbp.pop(i)\n",
    "\n",
    "for i in range(0,len(sbp) - len([el for el in sbp if len(el) == 0])):\n",
    "    if(len(sbp[i])==0):\n",
    "        sbp.pop(i)\n",
    "\n",
    "for i in range(0,len(bt) - len([el for el in bt if len(el) == 0])):\n",
    "    if(len(bt[i])==0):\n",
    "        bt.pop(i)\n",
    "\n",
    "for i in range(0,len(hr) - len([el for el in hr if len(el) == 0])):\n",
    "    if(len(hr[i])==0):\n",
    "        hr.pop(i)\n",
    "\n",
    "for i in range(0,len(rr) - len([el for el in rr if len(el) == 0])):\n",
    "    if(len(rr[i])==0):\n",
    "        rr.pop(i)\n",
    "\n",
    "for i in range(0,len(ecg) - len([el for el in ecg if len(el) == 0])):\n",
    "    if(len(ecg[i])==0):\n",
    "        ecg.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists should have the same length i.e. len(caseids)\n",
    "# Normalization can be done with a keras layer from preprocessing library\n",
    "old_settings = np.seterr(all='raise')\n",
    "idx_remove = []\n",
    "for i in range(0,len(ecg)): \n",
    "    try:\n",
    "        #Remove negative values\n",
    "        ecg[i][np.argwhere(ecg[i]<0)] = np.mean(ecg[i]) \n",
    "        # MinMax normalization\n",
    "        ecg[i] = (ecg[i] - np.min(ecg[i]))/(np.max(ecg[i])-np.min(ecg[i])) \n",
    "    except: \n",
    "        # remove values for which the normalization gives Runtime warning\n",
    "        idx_remove.append(i)\n",
    "\n",
    "for idx in idx_remove: \n",
    "    ecg.pop(idx)\n",
    "\n",
    "idx_remove = []\n",
    "for i in range(0,len(dbp)): \n",
    "    try:\n",
    "        #Remove negative values\n",
    "        dbp[i][np.argwhere(dbp[i]<0)] = np.mean(dbp[i]) \n",
    "        # MinMax normalization\n",
    "        dbp[i] = (dbp[i] - np.min(dbp[i]))/(np.max(dbp[i])-np.min(dbp[i])) \n",
    "    except: \n",
    "        # remove values for which the normalization gives Runtime warning\n",
    "        idx_remove.append(i)\n",
    "\n",
    "for idx in idx_remove: \n",
    "    dbp.pop(idx)\n",
    "\n",
    "idx_remove = []  \n",
    "for i in range(0,len(sbp)): \n",
    "    try:\n",
    "        #Remove negative values\n",
    "        sbp[i][np.argwhere(sbp[i]<0)] = np.mean(sbp[i]) \n",
    "        # MinMax normalization\n",
    "        sbp[i] = (sbp[i] - np.min(sbp[i]))/(np.max(sbp[i])-np.min(sbp[i])) \n",
    "    except: \n",
    "        # remove values for which the normalization gives Runtime warning\n",
    "        idx_remove.append(i)\n",
    "\n",
    "for idx in idx_remove: \n",
    "    sbp.pop(idx)\n",
    "\n",
    "\n",
    "\n",
    "idx_remove = []\n",
    "for i in range(0,len(bt)): \n",
    "    try:\n",
    "        #Remove negative values\n",
    "        bt[i][np.argwhere(bt[i]<0)] = np.mean(bt[i]) \n",
    "        # MinMax normalization\n",
    "        bt[i] = (bt[i] - np.min(bt[i]))/(np.max(bt[i])-np.min(bt[i])) \n",
    "    except: \n",
    "        # remove values for which the normalization gives Runtime warning\n",
    "        idx_remove.append(i)\n",
    "\n",
    "for idx in idx_remove: \n",
    "    bt.pop(idx)\n",
    "\n",
    "\n",
    "\n",
    "idx_remove = []\n",
    "for i in range(0,len(hr)): \n",
    "    try:\n",
    "        #Remove negative values\n",
    "        hr[i][np.argwhere(hr[i]<0)] = np.mean(hr[i]) \n",
    "        # MinMax normalization\n",
    "        hr[i] = (hr[i] - np.min(hr[i]))/(np.max(hr[i])-np.min(hr[i])) \n",
    "    except: \n",
    "        # remove values for which the normalization gives Runtime warning\n",
    "        idx_remove.append(i)\n",
    "\n",
    "for idx in idx_remove: \n",
    "    hr.pop(idx)\n",
    "\n",
    "\n",
    "idx_remove = []\n",
    "for i in range(0,len(rr)): \n",
    "    try:\n",
    "        #Remove negative values\n",
    "        rr[i][np.argwhere(rr[i]<0)] = np.mean(rr[i]) \n",
    "        # MinMax normalization\n",
    "        rr[i] = (rr[i] - np.min(rr[i]))/(np.max(rr[i])-np.min(rr[i])) \n",
    "    except: \n",
    "        # remove values for which the normalization gives Runtime warning\n",
    "        idx_remove.append(i)\n",
    "\n",
    "for idx in idx_remove: \n",
    "    rr.pop(idx)\n",
    "\n",
    "\n",
    "# Back to default settings for errors\n",
    "np.seterr(**old_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.argmax([len(el) for el in sbp])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(511)\n",
    "plt.title(\"DBP\")\n",
    "plt.plot(dbp[p], color='b')\n",
    "plt.subplots_adjust(hspace=1.)\n",
    "plt.subplot(512)\n",
    "plt.title(\"SBP\")\n",
    "plt.plot(sbp[p][:], color='r')\n",
    "\n",
    "plt.subplot(513)\n",
    "plt.title(\"Body temperature\")\n",
    "plt.plot(bt[p][:], color='orange')\n",
    "\n",
    "plt.subplot(514)\n",
    "plt.title(\"Heart rate\")\n",
    "plt.plot(hr[p][:], color='r')\n",
    "\n",
    "plt.subplot(515)\n",
    "plt.title(\"Respiratory rate\")\n",
    "plt.plot(rr[p][:], color='g')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read unhealthy patient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "caseids_all = vitaldb.find_cases(['ECG_II','ART_DBP','ART_SBP','BT','HR','RR']) # find ids of patient with this parameters\n",
    "# Load dataset\n",
    "df = pd.read_csv('https://api.vitaldb.net/cases')\n",
    "df = df[df['asa'] > 3]\n",
    "# df = df[df['death_inhosp'] == False] \n",
    "\n",
    "caseids_unhealthy = df['caseid'].to_numpy() \n",
    "\n",
    "caseids = [el for el in caseids_all if el in caseids_unhealthy]\n",
    "\n",
    "dbp = []\n",
    "sbp = []\n",
    "# load all the patients data into bp\n",
    "for i in range(0,5):\n",
    "    vals = vitaldb.load_case(caseids[i], ['ART_DBP','ART_SBP'])\n",
    "    dbp.append(vals[:,0])\n",
    "    sbp.append(vals[:,1])\n",
    "    dbp[i] = dbp[i][~np.isnan(dbp[i])] # extract non-null values of dyastolic pressure\n",
    "    sbp[i] = sbp[i][~np.isnan(sbp[i])] # extract non-null values of systolic pressure\n",
    "    sbp[i][np.argwhere(sbp[i]<0)] = np.mean(sbp[i]) # Threshold on 50 and then substitute with the mean value of the single patient data\n",
    "    dbp[i][np.argwhere(dbp[i]<0)] = np.mean(dbp[i]) # Threshold on 50 and then substitute with the mean value of the single patient data\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(211)\n",
    "plt.plot(dbp[4][:], color='b')\n",
    "plt.subplot(212)\n",
    "plt.plot(sbp[4][:], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = np.asarray([item for sublist in dbp for item in sublist],dtype='float64')\n",
    "print(flat_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = np.array_split(flat_list,1000)\n",
    "seq_len = len(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = int(len(chunks) * 0.7) # 70% percent of data fro training\n",
    "max_lenght = max([len(el) for el in chunks])\n",
    "# Uniform lenghts\n",
    "X_train = pad_sequences(chunks, max_lenght,padding='post',value=0.5,dtype='float64')\n",
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = X_train.shape[1]\n",
    "#input Layer\n",
    "input_layer = tf.keras.layers.Input(shape=(n_dim,))\n",
    "#Encoder\n",
    "encoder = tf.keras.layers.Dense(512, activation=\"tanh\", activity_regularizer=tf.keras.regularizers.l2(0.0005))(input_layer)\n",
    "encoder=tf.keras.layers.Dropout(0.2)(encoder)\n",
    "encoder = tf.keras.layers.Dense(256, activation='tanh')(encoder)\n",
    "encoder = tf.keras.layers.Dense(128, activation='relu')(encoder)\n",
    "encoder = tf.keras.layers.Dense(64, activation='relu')(encoder)\n",
    "# Decoder\n",
    "decoder = tf.keras.layers.Dense(64, activation='relu')(encoder)\n",
    "decoder = tf.keras.layers.Dense(128, activation='relu')(decoder)\n",
    "decoder = tf.keras.layers.Dense(256, activation='tanh')(decoder)\n",
    "decoder=tf.keras.layers.Dropout(0.2)(decoder)\n",
    "decoder = tf.keras.layers.Dense(512, activation='tanh')(decoder)\n",
    "decoder = tf.keras.layers.Dense(n_dim, activation='tanh')(decoder)\n",
    "#Autoencoder\n",
    "autoencoder = tf.keras.Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train,X_train,epochs=15,batch_size=64,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_p = vitaldb.load_case(263, ['ART_DBP','ART_SBP','BT','HR','RR'])\n",
    "test = val_p[:,0]\n",
    "\n",
    "test = test[~np.isnan(test)] \n",
    "\n",
    "print(np.shape(test))\n",
    "\n",
    "test = test[:seq_len]\n",
    "test = (test-np.min(test))/(np.max(test)-np.min(test))\n",
    "\n",
    "\n",
    "res = autoencoder.predict(np.expand_dims(test,axis=0))\n",
    "\n",
    "print(np.linalg.norm(test,2))\n",
    "print(np.linalg.norm(res[0],2))\n",
    "print(abs(np.linalg.norm(test,2)-np.linalg.norm(res[0],2)))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(211)\n",
    "plt.title('Original')\n",
    "plt.plot(test)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title('Reconstructed')\n",
    "\n",
    "plt.plot(res[0],scaley=1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a patient. NB: the data has to be normalized with MinMax approach\n",
    "#val_p = vitaldb.load_case(17, ['ART_DBP','ART_SBP','BT','HR','RR'])\n",
    "#\n",
    "## Pre-processing: remove nan vals, normalize and padding\n",
    "#val_p = val_p[~np.isnan(val_p)]\n",
    "#val_p = (val_p-np.min(val_p))/(np.max(val_p)-np.min(val_p))\n",
    "#test = pad_sequences([val_p],max_lenght,padding='post',value=0.5,dtype='float64')\n",
    "#\n",
    "#res = autoencoder.predict(test)\n",
    "#\n",
    "## The error for case 464 (ill patient) is much greater than case 17 (healthy patient)\n",
    "#print(np.linalg.norm(test,2))\n",
    "#print(np.linalg.norm(res[0],2))\n",
    "#print(abs(np.linalg.norm(test,2)-np.linalg.norm(res[0],2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Autoencoder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = np.asarray([item for sublist in dbp for item in sublist],dtype='float64')\n",
    "print(flat_list.shape)\n",
    "TIME_STEP = 6000 # 6000 is the mean length of the dbp time series\n",
    "X_train = []\n",
    "for seq in range(0,len(flat_list), TIME_STEP):\n",
    "    X_train.append(flat_list[seq:seq+TIME_STEP])\n",
    "X_train = np.asarray(X_train,dtype=object)\n",
    "X_train = pad_sequences(X_train, TIME_STEP,padding='post',value=0.5,dtype='float64')\n",
    "X_train = np.asarray(np.expand_dims(X_train,axis=2))\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, n_features= X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_len, n_features),return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(64,return_sequences=True,recurrent_activation='tanh'))\n",
    "model.add(LSTM(32,return_sequences=True,recurrent_activation='relu'))\n",
    "model.add(LSTM(32,return_sequences=True,recurrent_activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=True,recurrent_activation='tanh'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(128,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, X_train, epochs=10, batch_size=64, validation_split=0.1,shuffle=False,verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_dbp = np.asarray([item for sublist in dbp for item in sublist],dtype='float64')\n",
    "flat_list_sbp = np.asarray([item for sublist in sbp for item in sublist],dtype='float64')\n",
    "flat_list_hr = np.asarray([item for sublist in hr for item in sublist],dtype='float64')\n",
    "flat_list_bt = np.asarray([item for sublist in bt  for item in sublist],dtype='float64')\n",
    "flat_list_rr = np.asarray([item for sublist in rr for item in sublist],dtype='float64')\n",
    "print({'dbp':len(flat_list_dbp),'sbp':len(flat_list_sbp),'hr':len(flat_list_hr),'bt':len(flat_list_bt),'rr':len(flat_list_rr)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP = 6000 # 6000 is the mean length of the dbp time series\n",
    "X_train = []\n",
    "for seq in range(0,len(flat_list_dbp), TIME_STEP):\n",
    "    X_train.append(flat_list_dbp[seq:seq+TIME_STEP])\n",
    "X_train = np.asarray(X_train,dtype=object)\n",
    "X_train = pad_sequences(X_train, TIME_STEP,padding='post',value=0.5,dtype='float64')\n",
    "X_train = np.asarray(np.expand_dims(X_train,axis=2))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flsbp = np.asarray([item for sublist in sbp for item in sublist],dtype='float64')\n",
    "\n",
    "Xsbp = []\n",
    "for seq in range(0,len(flsbp), TIME_STEP):\n",
    "    Xsbp.append(flsbp[seq:seq+TIME_STEP])\n",
    "Xsbp = np.asarray(Xsbp,dtype=object)\n",
    "Xsbp = pad_sequences(Xsbp, TIME_STEP,padding='post',value=0.5,dtype='float64')\n",
    "print(Xsbp.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sbp = Xsbp[:2945,:]\n",
    "Z = np.concatenate([X_train,X_sbp[...,None]],axis=2)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Z[:10,...]\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, n_features= Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(seq_len, n_features),return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(32,return_sequences=True,recurrent_activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=True,recurrent_activation='tanh'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Y, Y, epochs=2, validation_split=0.1,shuffle=False,verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83f3cce98b4a0628e00255681f870890cb90ecb6f8db2750deab685955efac63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
