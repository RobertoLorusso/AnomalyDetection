{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vitaldb\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API data load and save on disk\n",
    "\n",
    "Load and save the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read healthy patients data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m caseids_all \u001b[39m=\u001b[39m vitaldb\u001b[39m.\u001b[39;49mfind_cases([\u001b[39m'\u001b[39;49m\u001b[39mECG_II\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mART_DBP\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mART_SBP\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mBT\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mHR\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mRR\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m# find ids of patient with this parameters\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Load dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mhttps://api.vitaldb.net/cases\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/vitaldb/dataset.py:63\u001b[0m, in \u001b[0;36mfind_cases\u001b[0;34m(track_names)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mglobal\u001b[39;00m dftrks\n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m dftrks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     dftrks \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://api.vitaldb.net/trks\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(track_names, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m track_names\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1965\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(byte_view))\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/siia/lib/python3.9/gzip.py:495\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m    493\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread(io\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> 495\u001b[0m uncompress \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(buf, size)\n\u001b[1;32m    496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail \u001b[39m!=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    497\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mprepend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "caseids_all = vitaldb.find_cases(['ECG_II','ART_DBP','ART_SBP','BT','HR','RR']) # find ids of patient with this parameters\n",
    "# Load dataset\n",
    "df = pd.read_csv('https://api.vitaldb.net/cases')\n",
    "df = df[df['asa'] < 3] # Extract ids of patients with asa < 3 and not dead in hospital\n",
    "df = df[df['death_inhosp'] == False]\n",
    "\n",
    "caseids_healthy = df['caseid'].to_numpy() # extract the case ids of patience with good health\n",
    "caseids = [el for el in caseids_all if el in caseids_healthy]\n",
    "print(len(caseids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = []\n",
    "dbp = []\n",
    "sbp = []\n",
    "bt  = []\n",
    "hr  = []\n",
    "rr  = []\n",
    "# load all the patients data \n",
    "for i in range(0,len(caseids)): # Select only five patient for testing purpose; then len(caseids)\n",
    "    try:\n",
    "        vals = vitaldb.load_case(caseids[i], ['ART_DBP','ART_SBP','BT','HR','RR'])\n",
    "        dbp.append(vals[:,0])\n",
    "        sbp.append(vals[:,1])\n",
    "        bt.append(vals[:,2])\n",
    "        hr.append(vals[:,3])\n",
    "        rr.append(vals[:,4])\n",
    "\n",
    "        # extract non-null values\n",
    "        dbp[i] = dbp[i][~np.isnan(dbp[i])]  \n",
    "        sbp[i] = sbp[i][~np.isnan(sbp[i])] \n",
    "        bt[i] = bt[i][~np.isnan(bt[i])]\n",
    "        hr[i] = hr[i][~np.isnan(hr[i])] \n",
    "        rr[i] = rr[i][~np.isnan(rr[i])]\n",
    "    \n",
    "    except Exception as e: \n",
    "        print('\\n=================\\n')\n",
    "        print('INDEX: '+str(i))\n",
    "        print('ERROR: '+str(type(e)))\n",
    "        print('\\n=================\\n')\n",
    "        pass\n",
    "\n",
    "\n",
    "for i in range(0,len(caseids)):\n",
    "    #vals = vitaldb.load_case(caseids[i], ['ECG_II'], 0.01) #parameter 0.01 for a 'zoomed' ecg\n",
    "    vals = vitaldb.load_case(caseids[i], ['ECG_II'])\n",
    "    ecg.append(vals[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "filepath = '/Users/Roberto/projects/siiaproject-vitanomaly/data.vitaldb'\n",
    "ecgpath = '/Volumes/Windows/SIIA/ecg_data.vitaldb'\n",
    "ecgpath = '/Users/Roberto/projects/siiaproject-vitanomaly/ecg_data.vitaldb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "\n",
    "with open(filepath, 'wb') as f:\n",
    "    pickle.dump((dbp,sbp,bt,hr,rr), f)\n",
    "\n",
    "with open(ecgpath, 'wb') as f:\n",
    "    pickle.dump(ecg, f)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = []\n",
    "dbp = []\n",
    "sbp = []\n",
    "bt  = []\n",
    "hr  = []\n",
    "rr  = []\n",
    "\n",
    "# save the data into a file since loading all the 2k caseids requires at least 1h\n",
    "filepath = '/Users/Roberto/projects/AnomalyDetection/data/raw/numeric_data.vitaldb'\n",
    "ecgpath = '/Volumes/Windows/SIIA/ecg_data.vitaldb'\n",
    "ecgpath = '/Users/Roberto/projects/AnomalyDetection/data/raw/ecg_data.vitaldb'\n",
    "\n",
    "with open(ecgpath, 'rb') as f:\n",
    "    (ecg) = pickle.load(f)\n",
    "with open(filepath, 'rb') as f:\n",
    "    (dbp,sbp,bt,hr,rr) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty elements\n",
    "\n",
    "for i in range(0,len(dbp) - len([el for el in dbp if len(el) == 0])):\n",
    "    if(len(dbp[i])==0):\n",
    "        dbp.pop(i)\n",
    "\n",
    "for i in range(0,len(sbp) - len([el for el in sbp if len(el) == 0])):\n",
    "    if(len(sbp[i])==0):\n",
    "        sbp.pop(i)\n",
    "\n",
    "for i in range(0,len(bt) - len([el for el in bt if len(el) == 0])):\n",
    "    if(len(bt[i])==0):\n",
    "        bt.pop(i)\n",
    "\n",
    "for i in range(0,len(hr) - len([el for el in hr if len(el) == 0])):\n",
    "    if(len(hr[i])==0):\n",
    "        hr.pop(i)\n",
    "\n",
    "for i in range(0,len(rr) - len([el for el in rr if len(el) == 0])):\n",
    "    if(len(rr[i])==0):\n",
    "        rr.pop(i)\n",
    "\n",
    "for i in range(0,len(ecg) - len([el for el in ecg if len(el) == 0])):\n",
    "    if(len(ecg[i])==0):\n",
    "        ecg.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'raise', 'over': 'raise', 'under': 'raise', 'invalid': 'raise'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lists should have the same length i.e. len(caseids)\n",
    "# Normalization can be done with a keras layer from preprocessing library\n",
    "old_settings = np.seterr(all='raise')\n",
    "idx_remove = []\n",
    "for i in range(0,len(ecg)): \n",
    "    try:\n",
    "        #Remove negative values\n",
    "        ecg[i][np.argwhere(ecg[i]<0)] = np.mean(ecg[i]) \n",
    "        # MinMax normalization\n",
    "        ecg[i] = (ecg[i] - np.min(ecg[i]))/(np.max(ecg[i])-np.min(ecg[i])) \n",
    "    except: \n",
    "        # remove values for which the normalization gives Runtime warning\n",
    "        idx_remove.append(i)\n",
    "\n",
    "for idx in idx_remove: \n",
    "    ecg.pop(idx)\n",
    "\n",
    "idx_remove = []\n",
    "for i in range(0,len(dbp)): \n",
    "    try:\n",
    "        #Remove negative values\n",
    "        dbp[i][np.argwhere(dbp[i]<0)] = np.mean(dbp[i]) \n",
    "        # MinMax normalization\n",
    "        dbp[i] = (dbp[i] - np.min(dbp[i]))/(np.max(dbp[i])-np.min(dbp[i])) \n",
    "    except: \n",
    "        # remove values for which the normalization gives Runtime warning\n",
    "        idx_remove.append(i)\n",
    "\n",
    "for idx in idx_remove: \n",
    "    dbp.pop(idx)\n",
    "\n",
    "idx_remove = []  \n",
    "for i in range(0,len(sbp)): \n",
    "    try:\n",
    "        #Remove negative values\n",
    "        sbp[i][np.argwhere(sbp[i]<0)] = np.mean(sbp[i]) \n",
    "        # MinMax normalization\n",
    "        sbp[i] = (sbp[i] - np.min(sbp[i]))/(np.max(sbp[i])-np.min(sbp[i])) \n",
    "    except: \n",
    "        # remove values for which the normalization gives Runtime warning\n",
    "        idx_remove.append(i)\n",
    "\n",
    "for idx in idx_remove: \n",
    "    sbp.pop(idx)\n",
    "\n",
    "\n",
    "\n",
    "idx_remove = []\n",
    "for i in range(0,len(bt)): \n",
    "    try:\n",
    "        #Remove negative values\n",
    "        bt[i][np.argwhere(bt[i]<0)] = np.mean(bt[i]) \n",
    "        # MinMax normalization\n",
    "        bt[i] = (bt[i] - np.min(bt[i]))/(np.max(bt[i])-np.min(bt[i])) \n",
    "    except: \n",
    "        # remove values for which the normalization gives Runtime warning\n",
    "        idx_remove.append(i)\n",
    "\n",
    "for idx in idx_remove: \n",
    "    bt.pop(idx)\n",
    "\n",
    "\n",
    "\n",
    "idx_remove = []\n",
    "for i in range(0,len(hr)): \n",
    "    try:\n",
    "        #Remove negative values\n",
    "        hr[i][np.argwhere(hr[i]<0)] = np.mean(hr[i]) \n",
    "        # MinMax normalization\n",
    "        hr[i] = (hr[i] - np.min(hr[i]))/(np.max(hr[i])-np.min(hr[i])) \n",
    "    except: \n",
    "        # remove values for which the normalization gives Runtime warning\n",
    "        idx_remove.append(i)\n",
    "\n",
    "for idx in idx_remove: \n",
    "    hr.pop(idx)\n",
    "\n",
    "\n",
    "idx_remove = []\n",
    "for i in range(0,len(rr)): \n",
    "    try:\n",
    "        #Remove negative values\n",
    "        rr[i][np.argwhere(rr[i]<0)] = np.mean(rr[i]) \n",
    "        # MinMax normalization\n",
    "        rr[i] = (rr[i] - np.min(rr[i]))/(np.max(rr[i])-np.min(rr[i])) \n",
    "    except: \n",
    "        # remove values for which the normalization gives Runtime warning\n",
    "        idx_remove.append(i)\n",
    "\n",
    "for idx in idx_remove: \n",
    "    rr.pop(idx)\n",
    "\n",
    "\n",
    "# Back to default settings for errors\n",
    "np.seterr(**old_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.argmax([len(el) for el in sbp])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(511)\n",
    "plt.title(\"DBP\")\n",
    "plt.plot(dbp[p], color='b')\n",
    "plt.subplots_adjust(hspace=1.)\n",
    "plt.subplot(512)\n",
    "plt.title(\"SBP\")\n",
    "plt.plot(sbp[p][:], color='r')\n",
    "\n",
    "plt.subplot(513)\n",
    "plt.title(\"Body temperature\")\n",
    "plt.plot(bt[p][:], color='orange')\n",
    "\n",
    "plt.subplot(514)\n",
    "plt.title(\"Heart rate\")\n",
    "plt.plot(hr[p][:], color='r')\n",
    "\n",
    "plt.subplot(515)\n",
    "plt.title(\"Respiratory rate\")\n",
    "plt.plot(rr[p][:], color='g')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = np.asarray([item for sublist in dbp for item in sublist],dtype='float64')\n",
    "print(flat_list.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = np.array_split(flat_list,1000)\n",
    "seq_len = len(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ntrain = int(len(chunks) * 0.7) # 70% percent of data fro training\n",
    "max_lenght = max([len(el) for el in chunks])\n",
    "# Uniform lenghts\n",
    "X_train = pad_sequences(chunks, max_lenght,padding='post',value=0.5,dtype='float64')\n",
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = X_train.shape[1]\n",
    "#input Layer\n",
    "input_layer = tf.keras.layers.Input(shape=(n_dim,))\n",
    "#Encoder\n",
    "encoder = tf.keras.layers.Dense(512, activation=\"tanh\", activity_regularizer=tf.keras.regularizers.l2(0.0005))(input_layer)\n",
    "encoder=tf.keras.layers.Dropout(0.2)(encoder)\n",
    "encoder = tf.keras.layers.Dense(256, activation='tanh')(encoder)\n",
    "encoder = tf.keras.layers.Dense(128, activation='relu')(encoder)\n",
    "encoder = tf.keras.layers.Dense(64, activation='relu')(encoder)\n",
    "# Decoder\n",
    "decoder = tf.keras.layers.Dense(64, activation='relu')(encoder)\n",
    "decoder = tf.keras.layers.Dense(128, activation='relu')(decoder)\n",
    "decoder = tf.keras.layers.Dense(256, activation='tanh')(decoder)\n",
    "decoder=tf.keras.layers.Dropout(0.2)(decoder)\n",
    "decoder = tf.keras.layers.Dense(512, activation='tanh')(decoder)\n",
    "decoder = tf.keras.layers.Dense(n_dim, activation='tanh')(decoder)\n",
    "#Autoencoder\n",
    "autoencoder = tf.keras.Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train,X_train,epochs=15,batch_size=64,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_p = vitaldb.load_case(263, ['ART_DBP','ART_SBP','BT','HR','RR'])\n",
    "test = val_p[:,0]\n",
    "\n",
    "test = test[~np.isnan(test)] \n",
    "\n",
    "print(np.shape(test))\n",
    "\n",
    "test = test[:seq_len]\n",
    "test = (test-np.min(test))/(np.max(test)-np.min(test))\n",
    "\n",
    "\n",
    "res = autoencoder.predict(np.expand_dims(test,axis=0))\n",
    "\n",
    "print(np.linalg.norm(test,2))\n",
    "print(np.linalg.norm(res[0],2))\n",
    "print(abs(np.linalg.norm(test,2)-np.linalg.norm(res[0],2)))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(211)\n",
    "plt.title('Original')\n",
    "plt.plot(test)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title('Reconstructed')\n",
    "\n",
    "plt.plot(res[0],scaley=1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a patient. NB: the data has to be normalized with MinMax approach\n",
    "#val_p = vitaldb.load_case(17, ['ART_DBP','ART_SBP','BT','HR','RR'])\n",
    "#\n",
    "## Pre-processing: remove nan vals, normalize and padding\n",
    "#val_p = val_p[~np.isnan(val_p)]\n",
    "#val_p = (val_p-np.min(val_p))/(np.max(val_p)-np.min(val_p))\n",
    "#test = pad_sequences([val_p],max_lenght,padding='post',value=0.5,dtype='float64')\n",
    "#\n",
    "#res = autoencoder.predict(test)\n",
    "#\n",
    "## The error for case 464 (ill patient) is much greater than case 17 (healthy patient)\n",
    "#print(np.linalg.norm(test,2))\n",
    "#print(np.linalg.norm(res[0],2))\n",
    "#print(abs(np.linalg.norm(test,2)-np.linalg.norm(res[0],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read UNHEALTHY patients data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "caseids_all = vitaldb.find_cases(['ART_DBP','ART_SBP']) # find ids of patient with this parameters\n",
    "# Load dataset\n",
    "df = pd.read_csv('https://api.vitaldb.net/cases')\n",
    "df = df[df['asa'] > 3]\n",
    "# df = df[df['death_inhosp'] == False] \n",
    "\n",
    "caseids_unhealthy = df['caseid'].to_numpy() #extract the case ids of patience with good health\n",
    "\n",
    "caseids = [el for el in caseids_all if el in caseids_unhealthy]\n",
    "\n",
    "dbp = []\n",
    "sbp = []\n",
    "# load all the patients data into bp\n",
    "for i in range(0,5):\n",
    "    vals = vitaldb.load_case(caseids[i], ['ART_DBP','ART_SBP'])\n",
    "    dbp.append(vals[:,0])\n",
    "    sbp.append(vals[:,1])\n",
    "    dbp[i] = dbp[i][~np.isnan(dbp[i])] # extract non-null values of dyastolic pressure\n",
    "    sbp[i] = sbp[i][~np.isnan(sbp[i])] # extract non-null values of systolic pressure\n",
    "    sbp[i][np.argwhere(sbp[i]<0)] = np.mean(sbp[i]) # Threshold on 50 and then substitute with the mean value of the single patient data\n",
    "    dbp[i][np.argwhere(dbp[i]<0)] = np.mean(dbp[i]) # Threshold on 50 and then substitute with the mean value of the single patient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(211)\n",
    "plt.plot(dbp[4][:], color='b')\n",
    "plt.subplot(212)\n",
    "plt.plot(sbp[4][:], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM AUTOENCODER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 17666, 1)\n"
     ]
    }
   ],
   "source": [
    "flat_list = np.asarray([item for sublist in dbp for item in sublist],dtype='float64')\n",
    "chunks = np.array_split(flat_list,1000)\n",
    "max_lenght = max([len(el) for el in chunks])\n",
    "X_train = pad_sequences(chunks, max_lenght,padding='post',value=0.5,dtype='float64')\n",
    "X_train = np.asarray(np.expand_dims(X_train,axis=2))\n",
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_22 (LSTM)              (None, 17666, 128)        66560     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 17666, 128)        0         \n",
      "                                                                 \n",
      " lstm_23 (LSTM)              (None, 17666, 64)         49408     \n",
      "                                                                 \n",
      " lstm_24 (LSTM)              (None, 17666, 32)         12416     \n",
      "                                                                 \n",
      " lstm_25 (LSTM)              (None, 17666, 32)         8320      \n",
      "                                                                 \n",
      " lstm_26 (LSTM)              (None, 17666, 64)         24832     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 17666, 64)         0         \n",
      "                                                                 \n",
      " lstm_27 (LSTM)              (None, 17666, 128)        98816     \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 17666, 1)         129       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 260,481\n",
      "Trainable params: 260,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1],X_train.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(64,return_sequences=True,recurrent_activation='tanh'))\n",
    "model.add(LSTM(32,return_sequences=True,recurrent_activation='relu'))\n",
    "model.add(LSTM(32,return_sequences=True,recurrent_activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=True,recurrent_activation='tanh'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(128,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1/15 [=>............................] - ETA: 1:02:49 - loss: 0.3266"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, X_train, epochs=10, batch_size=64, validation_split=0.1,shuffle=False,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83f3cce98b4a0628e00255681f870890cb90ecb6f8db2750deab685955efac63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
